{"version":"2.2.0","option":{"annote":{"autosave":true,"color":1,"effect":"normal","event":"click","exp_mode":"both","exp_name":"html","export":true,"hypothes":false,"keyboard":"Q Q","link":"[{{an_text}}]({{an_int_uri}})","off":false,"order":"auto","savelocal":false,"shortcuts":"M M","style":"0","trigger":true},"auto_mode":"read","automated":[{"action":[{"id":1663756989374,"value":"html","disable":false}],"bgColor":"#195bf7","color":"#fff","icon":"<i class=\"fas fa-project-diagram\"></i>","id":1663756989374,"name":"","target":"unreader","type":"auto"},{"id":1695108339692,"name":"","color":"#fff","bgColor":"#195bf7","icon":"<i class=\"fas fa-project-diagram\"></i>","type":"auto","target":"annote","action":[{"id":1695108350614,"value":"html"}]}],"blacklist":["google.com","youtube.com","simp.red","simpread.herokuapp.com","simpread-test.herokuapp.com","simpread.ksria.cn","https://www.oschina.net/"],"br_exit":false,"create":"","darkmode":"manual","esc":true,"export":{"custom":[],"html":{"layout":"normal","proxy":true,"toc":true},"markdeep":{"custom":false,"template":""},"markdown":{"backlink":"","custom":false,"ext_uri":"","format":"","int_uri":"","single":false,"single_tmp":"","tag":"","tag_sep":"","template":""},"title":{"custom":true,"template":"{{id}}{{un_title}}{{mode}}"},"webhooks":[]},"lazyload":["baidu.com","weibo.com","youtube.com"],"manual":{"mode":"read","select":true,"shortcuts":"S S","site":"confirm"},"menu":{"blacklist":false,"exclusion":false,"focus":true,"lazyload":false,"link":false,"list":true,"manual":true,"option":true,"read":true,"unrdist":true,"whitelist":false},"notice":true,"origins":[],"plugins":["1VQ19jCD8Z","7gcXYY4HAc","DH9l5jblPH","E2CUZ2agRP","HD9GmoatXd","N3EznnUXeV","PcmPCT9rgM","TmZSYtz5lA","UEzvAXNSe5","dtIMMLktg9","klGUASLasg","tMGXrU1v0U","tbjTx33iAA","xZVbwW4C5A","y8Mai5IBwN"],"preload":true,"remote":{"export":{"attach":"ofhtml","epub":true,"kindle":false,"mail":true,"pdf":true,"plain":true},"parse":true,"port":7026,"rss":true,"sync":true},"save_at":"jianguo","secret":true,"shortcuts":"enable","sync":"2025年02月17日 19:18:43","uninstall":true,"unreader":{"autoiframe":false,"bottom":true,"exports":{"markdown":false,"pdf":false},"mode":"iframe","offline":true,"openmode":"evergreen","sametheme":true,"savelocal":false,"savetype":"ofhtml","search":true,"shortcuts":"D D","show":"senior","sidebar":true,"theme":"night"},"update":"2025年02月17日 19:18:43","urlscheme":true,"version":"2017-04-03"},"focus":{"auto":false,"bgcolor":"rgba( 235, 235, 235, 0.9 )","controlbar":true,"highlight":true,"mask":true,"opacity":90,"shortcuts":"A S","version":"2016-12-29"},"read":{"actionbar":{"download":{"color":"#D4237A","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/download_icon.png","items":{},"name":"导出"},"dyslexia":{"color":"#90ee02","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/dyslexia_icon.png","items":{},"name":"无障碍"},"exit":{"icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/exit_icon.png","name":"关闭"},"fontfamily":{"color":"#9C27B0","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/fontfamily_icon.png","items":{"fontfamily_Hiragino Sans GB":{"color":"#9C27B0","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/fontfamily_hiragino_icon.png","name":"冬青黑体"},"fontfamily_Microsoft Yahei":{"color":"#9C27B0","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/fontfamily_yahei_icon.png","name":"微软雅黑"},"fontfamily_PingFang SC":{"color":"#9C27B0","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/fontfamily_pingfang_icon.png","name":"苹方"},"fontfamily_Source Han Sans CN":{"color":"#9C27B0","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/fontfamily_hansans_icon.png","name":"思源黑体"},"fontfamily_default":{"color":"#9C27B0","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/fontfamily_default_icon.png","name":"系统默认"}},"name":"字体样式"},"fontsize":{"color":"#9E9E9E","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/fontsize_icon.png","items":{"fontsize_58%":{"color":"#9E9E9E","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/fontsize_small_icon.png","name":"减小"},"fontsize_62.5%":{"color":"#9E9E9E","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/fontsize_normal_icon.png","name":"正常"},"fontsize_70%":{"color":"#9E9E9E","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/fontsize_large_icon.png","name":"增大"}},"name":"字体大小"},"layout":{"color":"#FFEB3B","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/layout_icon.png","items":{"layout_15%":{"color":"#FFEB3B","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/layout_large_icon.png","name":"宽栏"},"layout_20%":{"color":"#FFEB3B","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/layout_normal_icon.png","name":"正常"},"layout_25%":{"color":"#FFEB3B","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/layout_small_icon.png","name":"窄栏"}},"name":"版面布局"},"option":{"color":"#03A9F4","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/option_icon.png","items":{},"name":"设定"},"readlater":{"color":"#FF5722","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/save_icon.png","items":{},"name":"暂存"},"send":{"color":"#00BCD4","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/send_icon.png","items":{"onenote":{"color":"#00BCD4","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/onenote_icon.png","name":"保存到 Onenote"},"yuque":{"color":"#00BCD4","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/yuque_icon.png","name":"语雀"}},"name":"保存"},"share":{"color":"#3f51b5","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/share_icon.png","items":{},"name":"共享"},"theme":{"color":"#FB8C00","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/theme_icon.png","items":{"theme_next":{"color":"#FB8C00","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/theme_next_icon.png","name":"后一个主题"},"theme_prev":{"color":"#FB8C00","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/theme_prev_icon.png","name":"前一个主题"}},"name":"主题"},"trigger":{"color":"#00bcd4","icon":"chrome-extension://ijllcpnolfcooahcekpamkbidhejabll/assets/images/plugin_icon.png","items":{},"name":"插件触发器"}},"auto":false,"cleanup":true,"controlbar":true,"custom":{"art":{"color":"","fontFamily":"","fontSize":"","fontWeight":"","letterSpacing":"","lineHeight":"","textIndent":"","wordSpacing":""},"code":{"fontFamily":"","fontSize":""},"css":"","desc":{"color":"","fontFamily":"","fontSize":""},"pre":{"textShadow":""},"title":{"color":"","fontFamily":"","fontSize":""}},"exclusion":["v2ex.com","issue.github.com","readme.github.com","question.zhihu.com","douban.com","nationalgeographic.com.cn","tech.163.com","docs.microsoft.com","msdn.microsoft.com","baijia.baidu.com","code.oschina.net","http://www.ifanr.com","http://www.ifanr.com/news","http://www.ifanr.com/app","http://www.ifanr.com/minapp","http://www.ifanr.com/dasheng","http://www.ifanr.com/data","https://www.ifanr.com/app","http://www.ifanr.com/weizhizao","http://www.thepaper.cn","http://www.pingwest.com","http://tech2ipo.com","https://www.waerfa.com/social","github.com"],"fap":true,"fap_layout":"list","fontfamily":"","fontsize":"60%","highlight":true,"layout":"30%","lazyimg":true,"progress":true,"pure":true,"shortcuts":"A A","theme":"github","toc":true,"toc_hide":true,"version":"2017-03-16","whitelist":["https://sspai.com/post/*"]},"websites":{"custom":[],"local":[["https://ped-yc.github.io/**/*",{"desc":"[[{$('meta[name=Description]').attr('content')||$('meta[name=description]').attr('content')}]]","exclude":[],"include":"[[`//*[@id='swup']/div[1]/div[2]/div[1]/div[1]/div[1]/div[3]`]]","name":"ped-yc.github.io","title":"<title>"}],["https://www.yuque.com/**/*",{"desc":"[[{$('meta[name=Description]').attr('content')||$('meta[name=description]').attr('content')}]]","exclude":[],"include":"[[`//*[@id='doc-reader-content']`]]","name":"www.yuque.com","title":"<title>"}],["https://developer.mozilla.org/zh-CN/docs/Web/CSS/Using_CSS_custom_properties#javascript_%E4%B8%AD%E7%9A%84%E5%80%BC",{"desc":"[[{$('meta[name=Description]').attr('content')||$('meta[name=description]').attr('content')}]]","exclude":[],"include":"[[`//*[@id='content']`]]","name":"developer.mozilla.org","title":"<title>"}],["https://dg.ped-yc.site/",{"desc":"[[{$('meta[name=Description]').attr('content')||$('meta[name=description]').attr('content')}]]","exclude":[],"include":"[[`//*[@id='quartz-body']/div[2]`]]","name":"dg.ped-yc.site","title":"<title>"}]],"person":[]},"statistics":{"read":152,"service":{"html":181,"markdown":149,"refslink":1,"save":3}},"user":{"avatar":"","contact":"","email":"zxc66war3@outlook.com","name":"风熏人醉","rule":2,"uid":"e65x4po0-hher-0joq-3ut3-jmubbt7rwslc"},"notice":{"latest":173,"read":[173,172,171,170,169,168,167,166,165,164,163,162,161,160,159,158,157,156,155,154,153,152,151,150,149,148,147,146,145,144,143,142,141,140,139,138,137,136,135,134,133,132,131,130,129,128,127,126,125,124,123,122,121,120,119,118,117,116,115,114,113,112,111,110,109,108,107,106,105,104,103,102,101,100,99,98,97,96,95,94,93,92,91,90,89,88,87,86,85,84,83,82,81,80,79,78,77,76,75,74,73,72,71,70,69,68,67,66,65,64,63,62,61,60,59,58,57,56,55,54,53,52,51,50,49,48,47,46,45,44,43,42,41,40,39,38,37,36,35,34,33,32,31,30,29,28,27,26,25,24,23,21,15,14,13,12,11,10,9,8,7,6,5,4,3,2,1],"unsee":0},"unrdist":[{"annotations":[{"annote":"<sr-annote data-id=\"1739786154416\">首先介绍一下业务的背景，这里主要从 3 个维度展开。第一个维度是组织维度，在立项之初，恰逢美团的多个事业群合并，因前端规模比较大，横向的流动协同比较多（需要跨部门支持需求，进行跨系统协作等等）。此外，美团到店事业群新人比例比较高，校招和新员工比例很高，我们会帮助新同学快速融入团队，需要完成一些较为基础的开发工作。</sr-annote>","color":1,"html":"首先介绍一下业务的背景，这里主要从 3 个维度展开。第一个维度是组织维度，在立项之初，恰逢美团的多个事业群合并，因前端规模比较大，横向的流动协同比较多（需要跨部门支持需求，进行跨系统协作等等）。此外，美团到店事业群新人比例比较高，校招和新员工比例很高，我们会帮助新同学快速融入团队，需要完成一些较为基础的开发工作。","id":1739786154416,"items":[{"root":"1 背景介绍\n1.1 业务背景\n\n首先介绍一下业务的背景，这里主要从 3 个维度展开。第一个维度是组织维度，在立项之初，恰逢美团的多个事业群合并，因前端规模比较大，横向的流动协同比较多（需要跨部门支持需求，进行跨系统协作等等）。此外，美团到店事业群新人比例比较高，校招和新员工比例很高，我们会帮助新同学快速融入团队，需要完成一些较为基础的开发工作。\n\n第二维度是业务维度，美团到店业务迭代频次比较高，基础工程框架不仅要保证交付速度快，同时还对质量有很高的要求。\n\n第三个维度是系统维度，因业务周期比较长，到店还存在大量的存量系统，需要考虑迁移升级和重构等问题，同时会有频繁的系统交接。\n\n1.2 技术背景\n\n在 Rome 整体立项时，我们已经准备好了相关的基础设施，包括发布系统的收敛、基础架构，统一为基于 S3（美团内部存储服务）加动静分离的技术架构，但是上层开发框架、组件类库种类繁多且开发方式不统一。存在问题包括：整个团队中人数比较多，学习交接、建设维护成本相对较高，而整体开发的效率比较低，跨团队之间的工程能力也很难进行复用等等。\n\n建设之初，我们基于纯静态 S3（美团内部存储服务）架构进行前端框架的建设，这源于我们早期大量基于 Node.js 的前后端一体架构存在一些问题：首先，事业群早期以中后台场景业务为主，对页面的秒开、SEO 的诉求比较低；其次，当时 Node.js 生态基建还没有那么完善，前端同学需要做动态扩缩容、峰值流量处理等操作，整体的业务风险比较高。同时还存在机器成本高、开发人员能力要求高、招聘难度大等问题。\n\n因此，在整体的建设思路和路径上，我们不会建设类 Egg.js 这样的前后端一体的框架；同时因为我们的框架层要解决研发流程不规范、交付质量不高等问题，也需要联动上下游的设计研发、CI/CD 等系统形成一体的开发工程平台，而不只是做 CLI 工具。​\n\n2 工程生态、演变路径和规模化升级\n2.1 工程生态\n2.1.1 降学习成本\n\n框架约束\n\n根据前文所述，我们一开始要解决的核心问题是学习成本，因此我们会做框架约束。\n\n第一是工程目录，我们在真正开发的时候，比如 UI 框架是使用 Vue 还是 React，UI 框架本身的语法成本没有那么高，但是当拿到一个需求时，应该在哪儿改，在哪个位置梳理逻辑，这个部分的成本比较高，因为很多时候不同技术栈的业务组织方式不一样，会带来比较高的理解成本和心智负担。\n第二个是大团队下工程能力碎片化，团队间水平比较参差，带来的问题是团队之间难以做好、做深工程能力，典型的如 SSR、构建优化等很难跨团队间复用，一直在重复造低门槛的 “轮子”。\n第三是业务工程的配置比较冗长，带来的问题是工程配置比较耗时，很多时候存量系统不敢升级，因为不知道这么长的配置加了一些东西，是否会出现比较严重的后果。\n最后是公司基建落地，业务对接过程中非标的基建落地实践版本非常多；在公司开发和自己写小型项目有很大的差异，在公司内要解决监控、流量、部署对接等问题，需要做周期升级，而这部分如果不进行整体收敛，整体规模化升级成本就会比较高。\n\nRome 在这一侧的解法：\n\n统一目录认知：如下图右侧所示，在开发时，对于部门或者团队级别认可的规范，当技术同学不遵守这套规范的时候，在代码提交时会有卡控，当然也可以通过 TL 审批跳过。\n收敛依赖选型和版本：在业务开发时，从头进行工程框架选型的上手门槛比较高，因此在公司内将版本和选型进行统一收敛，保证大家可以专注业务。\n统一跨技术栈工程能力和开闭配置：保证大家拿到工程后，不管是哪个技术栈都可以以一个相对一致的认知进行开发，无需关注构建、工程能力配置等。\n收敛基础开发规范：默认在拿到一个项目时，如果没有特别定制化的团队规范，基本上可以不用关注像 ESLint、Prettier、EditorConfig、部署对接等开发规范。\n\n跨技术栈开发认知一致\n\n在实际迭代的时候，到店内部一开始整体收敛到 Vue 技术栈，后来因团队合并，又有了 React 技术栈。我们采取的第一个策略是把 Vue 和 React 整体目录保持大家认知的一致，如下图所示。而在真正开发的时候，比如配置子目录 SRC 下，当一个同学一开始是偏 Vue，但开发 React 需求的时候，即使团队和业务有一些变动，TA 也可以准确拿到项目快速启动，接下来摆在 TA 面前的可能就是特定技术栈下，对于对应的 API 理解是否深入的问题，这部分同学自己就可以解决。\n\n第二是我们会保证使用 Rome 开发框架的整体开发调试流程和体验一致。如下图，在 Vue 或 React 框架中同学专注业务开发就行，基本上可以不用关注工程框架的相关事情。比如 Vite 在 Vue 或在 React 指令和表现是一致的，同学可以通过一行命令直接开始调试对应项目并进行开发。\n\n第三，我们的整体工程能力会进行高度对齐，如上图所示。我们生态下 80% 的插件在 Vue 和 React 下，用法和表现完全一致，大家基本上不用关心这部分，当真正用到某部分能力，详细看对应的文档就行。\n\n基础基建对接\n\n工程能力：一行代码引入公司基建（CDN 容灾）\n\n当前端同学处理如 CDN 厂商故障等问题时，需要自行查找、学习和使用公司 / 开源的基建能力，解决成本较高，但在 Rome 开发过程中可以通过开闭式的配置一键 CDN 容灾生效，如下图，接入后会自动进行静态资源的降级重试，当业务出现故障时（即下图峰值部分），它会有一个明显的资源加载重试，这就是降级 CDN 在生效；对业务同学来说只需配置框架需要哪个能力就行，其详细配置如何注入大家并不需要太关注。\n\n工程能力：一行代码引入公司基建（告警监控）\n\n业务开发以外的像告警监控，对同学来说，可能也要理解很多东西。但是在企业内开发，我们希望同学可以专注业务开发，像线上告警、日志链路等出现问题，可以不用配置对应平台，我们用对应的项目 Key 就可以查到对应的错误总量、错误调用链路等。\n\n工程能力：一行代码引入公司基建（水印）\n\n我们也可以一键完成对应的水印接入，同时开发配置时有对应的配置项智能提示。各种开闭能力的属性命名语义可能没那么清晰，可以通过如 VSCode 中的智能提示一键跳转到对应能力的文档地址，查看它的实现原理以及使用方法。\n\n2.1.2 降建设成本\n\n之前组织下建设的很多能力是偏小蘑菇的形式，“小蘑菇” 的语义是整体工程能力的生命周期比较短，会进行频繁的重复建设。但我们希望以一个大树的形式建设，“大树” 的语义是我们可以通过跨团队、横向方式进行高质量的设计和建设，通过评审委员会把控整体的方案建设，把方案拆分到几个阶段，横向团队共建，共同完成核心能力，比如 Serverless、SSR 等。\n\n工程能力生态\n\n如果大家对工程框架的建设能力感兴趣，可以参考我们内部的效率、质量、体验和领域实践进行对应分类的建设。我们通过一线调研、能力盘点的方式建设了 31 个，在企业内进行建设时，有很多能力一开始可能本身成本就很高，比如统一基建接入等，大概需要 30 人力 / 个，而实际业务需求开发一共只有几天时间，很难说服产品运营单独抽出大量人力开发前端基建能力，这部分我们就以共建 + 横向评审的方式，保证业务团队可以摊平整体的建设成本。\n\n典型共建案例\n\nServerless SSR 能力，在休娱频道的密室和优选团长端的订单管理流量分别是百万级别和千万级别，整体秒开可以做到 90% 左右。\n\n整个过程是 Rome + Arche 团队共建并形成到店标准，业务统一落地，过程中有超过 3 个业务团队参与了部分能力建设。\n\n这里简要介绍下 Serverless 的方案流程：\n\n首先是访问自动降级，用户通过负载均衡命中对应的网关，命中对应的 Serverless 的 Node Runtime，这里会有对应页面粒度的 SSR 渲染函数，当用户访问服务器返回 5.x.x 状态码或超时时会自动降级到 CSR 资源（SPA 页面）；即使出现了极端情况，比如 SSR 服务异常，我们也可以保证整体页面可用，差别主要在 SSR 场景秒开~ 90%，CSR 场景秒开~ 30%，兼顾了性能和稳定性的优势。\n\n其次是支持页面粒度接入，在大型 MPA 项目开发中，可以把某个页面单独设置为 SSR，不必要求整个项目都是 SSR 工程，构建时默认会上传 CSR 端构建资源，而针对 SSR 页面会额外进行另外一份资源的构建，在部署 SSR 端、CSR 端资源时分别部署，用户访问页面链接时，对应命中 Server 端或者 CDN 资源。\n\n支持自动扩缩容和流失渲染，对接公司 Nest 基建，支持业务无感知的峰值流量自动扩缩容；同时长列表页面也可以通过流式渲染来降低 TTFB 时间。\n\n2.1.3 提研发效率\n\n编译提速主要分两个方向进行业务落地，分别是 Webpack 体系和 Vite 体系。\n\n编译提速 - Webpack 体系优化\n\n首先是 Webpack 体系，因为我们整体的技术栈分 Vue 和 React 两部分，我们会抽一部分基础的跨技术栈的 Webpack 配置，上层有不同团队分别维护 Vue 技术栈和 React 技术栈的特有配置，中间我们会进行专项优化，包括像 Webpack5 的开发优化、编译压缩器、SWC 和 esbuild 等。\n\n另外，我们做工具时，不仅要考虑我们本身构建工具比如 Webpack，也要考虑如何和我们的 CI/CD 平台结合，如缓存复用结合依赖安装、构建工具做，在很多需求场景下，可以获得耗时的 10 倍提升，CI/CD 侧我们也支持分粒度的构建，比如大型 MPA 做页面粒度的构建等。\n\n编译提速 - 开发时⼀键 Vite\n\nVite 主要是我们在开发时使用的，Webpack 是构建时使用，除了少部分增量项目的开发和构建都走 Vite，大多数存量项目都是开发使用 Vite 保证效果，构建阶段使用 Webpack。\n\n整体流程：首先是启动，通过 Vite，比如 createServer 拉起页面，抹平 Webpack 差异；环境变量的抹平，保证客户端 Webpack 存量项目，将 Webpack 客户端的环境变量注入到 Vite 端；对配置文件，大家原本在 rome.config.js 中的 Webpack 配置也可以一键转换为对应的 Vite 配置；在下层我们会做内置的技术栈插件，包括 Vue2/Vue3 SFC+JSX，包括 react 的内置插件等；比较繁琐的是生态兼容，在公司内这部分的工作量比较大，像我们的一些 SaaS 的静态资源的一些路径等，包括之前我们存量 Webpack 这么多年积累下来的公司基建，我们如何保证它能被注入到 Vite 端，包括组件库按需引入，当一些模块比如 common.js 的模块如何指向对应的 esmodule 目录等，最后我们会屏蔽一些认知成本，把 Vue、React 和公司依赖包内置到 Vite 预编译内容中。\n\n依赖提速\n\n主要分两部分，分别是开发阶段和部署阶段。\n\n首先会做工程框架自身 Node.js 依赖的预构建，这部分核心解决的是 Node.js 端公司内外包资源的体积和递归依赖数量庞大的问题。依赖安装的一个核心耗时其实是递归依赖庞杂，需要逐个安装带来的耗时，比如需要安装 A，其实 A 依赖了 B、C、D、E、F、G，整体安装耗时就会非常久，同时研发同学对 Node.js 包体积、依赖数量一般不如面向浏览器端投放的 NPM 包那么敏感，这个时候预编译就很关键。\n\n其次是把整体的依赖环节包管理工具切换到 Pnpm。这部分经过调研（Pnpm、Yarn Berry、Yarn Berry with PNP 等），判断 Pnpm 是未来几年的核心趋势，同时可以稳定发展。内部建设了对应的包管理迁移工具，来服务企业内几百个、上千个项目。\n\n在部署阶段定制了 Rome 依赖安装 Docker 镜像，锁 Pnpm 版本和内置常用依赖、缓存复用，它的整体逻辑是上一次需求迭代和下一次需求迭代可能依赖包的变化没有那么多。我们在第二次发布的时候，如果能够尝试命中上一次的 node_modules 里的一些包，那第二次安装耗时就会有一个比较大的提升。\n\n2.2 演变路径\n\n第一阶段的时候是强依赖 Webpack 和 Vue 技术栈的。直到 2020 年 - 2021 年，像社区 UI 框架有 Solid.js、Svelte、Vue3，构建工具有 Esbuild、Rspack、Vite 等且发展势头也比较好，面向这个大趋势，我们希望研发框架能够分层过渡，不再和具体的 UI 框架、构建工具绑定：\n\n首先最底层是差异机制，核心做插件的加载和调度；\n中间层是构建工具的 bundler ，这一层主要是分别结合 Webpack、esbuild、Rspack 等构建实现，支持大量项目；\n上层 Plugin、Preset 做框架拓展，各个业务能够基于场景定制自己的工程能力；\n顶层是品牌指令（bin），这部分主要是企业内合作时业务方希望保持自己的品牌名。\n\n示例如下图右侧，大多数情况下，我们有一个新的研发框架出来，只需要开发我们对应的语言特色的插件集合就行，底层插件机制 + 构建工具大家其实可以保持一致的。\n\n2.3 规模化升级\n\n框架价值 = 覆盖范围 * 功能价值。我们的共识是建设的能力需要落地才能产生价值，核心是帮助大家业务低成本地享受好用的能力。\n\n如下图所示，是某个业务的升级成本图，可以看到有 100 多个项目（成本均值~ 5pd / 项目），业务面临几百天的成本。对团队 Tech Lead 很难说服业务产品运营业务阶段投入升级。对一线同学来说，虽然能力很多、很棒，但是升级风险怎么处理：编译提升 10s，线上问题定位消耗一周。\n\n这部分的我们给两个解决方案。\n\n首先是提供迁移工具（非 Rome 到 Rome 项目）：\n\n巨石项目做存量迁移时，一般同学下意识反应是通过 Babel 做静态解析迁移，纯静态解析实测的迁移准确率在 40%，也就说一个项目 40% 的事情通过 Babel 可以搞定。\n第二阶段是基于构建工具的依赖引用关系去做迁移，通过构建工具启动时的模块工厂钩子去生成依赖引用树状结构然后做迁移，得到迁移准确率在 60% 左右。\n第三阶段是动态分析结合 AST 静态分析一起去做，在数据采集阶段通过构建工具的模块工厂钩子、Vue/React 实例化对象的路由 / 数据流配置去生成依赖图结构，通过 recast 等工具进行静态解析，不用 Babel 是因为 recast 有比较好保持样式的代码回写，文件内容增删改后写回原文件样式不乱，同学在 git diff 时需要 Code Review 代码就可以少一些，便于迁移影响范围判断。\n最后配备上基础公司基建做的一些依赖版本、体积、兼容性检测的质量工具，自动迁移了差不多有 100 多个项目，线上零 Case。\n\n另一个是做框架生态的大版本升级，假如一个季度推出来一个大版本，实际业务开发升级难度、成本就会比较大，业务可能也不敢升级，这时大版本升级就要提供关键流程自动迁移支持：\n\n当从 Yarn 到 Pnpm 的时候，会做幽灵依赖检测，自动向 Pnpm/src 的幽灵依赖配置等；\n也会做工程源码对比；\n构建配置，如 Webpack 前后框架无关的配置对比；\n产物变动升级前后 entry 数量是否不一样、产物有没有明显变大；\n兼容性检测，升级前是 ES 5 + 的代码，升级后到 ES 6 了，那也会出问题；\n线上验证等。\n\n公司内部的一些标杆团队都是由我们 Rome 团队升级 + 写报告，标杆业务由基建团队升级的话，ROI 可能会比较高，我们升级通过工具将我们的二次调整，整体成本可能会非常低，2~4h 就可以完成一个中等存量项目，再由核心业务同学验收、线上灰度跟进等。\n\n3 框架开发辅助\n\n我们 Rome 的工程开发辅助工具是基于 IDE 的。一般业界的开发辅助工具两种形式，下图是 Vue UI 和 VSCode 的拓展图示：\n\n一是通过 Web 实现，典型案例有 Vue UI/Umi UI/Angular Console 等；\n二是 VSCode 拓展，如 Ice Works 等。\n3.1 为什么要基于 IDE？\n\n理解：对于 Web 形式，业界以 Umi UI（更先进）和 Vue CLI 为主，虽然它们都是基于 Web 实现，但出发点不太一样，Vue UI 核心是做体验提升，支持工程创建 CLI 可视化，工程配置分析等；但是 UMI UI 要服务公司内部，更多的是做提效，支持一些组的资产插入、执行工程任务比如 build/lint 等。\n\n基于 Web 做开发辅助工具\n\n好处：跨 IDE 的成本比较低，前端同学覆盖会比较全，也可以解决 IDE 内功能交互 UI 受限问题。\n\n问题：VSCode 不开放这部分的 UI 侵入能力，一方面心智认知类 Umi UI 的趋势其实把开发时的用户心智切换到 Web 端。但我们研发时，其实要在 IDE 看代码、理解代码、写代码，当我们热更新起来以后，像构建、物料插入、Link 等其实比较低频，就会带来一个问题，我们研发时的心智很难切到对应的浏览器端。另一方面研发链路，长期来看，不管是业界还是美团内部的研发趋势中就一部分场景，后续肯定是用 Cloud IDE，但是我们工程框架的 Web 端这种开发辅助工具，如果通过 Web 形式实现，就很难融入到我们的研发链路中。\n\nIDE 侧方案确实存在 UI 交互受限、视窗面积、跨 IDE 开发成本等问题，因为同学在开发时，VSCode 的面积就这么大，每弹出一个 Webview 就会侵占用户的开发视窗，会比较难受。\n\n选型原因：\n\n用户频次：我们核心是企业内做提效，那就决定了绕不开 IDE 侧，而且 TA 是用户高频部分，而且我们在代码侧也可以复用 VSCode、IDE 侧的 LS API，交互受限通过用户引导是可以接受的。\n研发整合：我们后续可以在 Cloud IDE 整体比较成熟的时候，会把部分轻量迭代切换到 Cloud IDE 中进行研发，同时希望新模式下开发辅助工具可以继续产生价值。第二是可以在 IDE 侧整合上下游链路，保证用户心智可以一直停留在 IDE 内而不用跨平台操作，整体成本可能会比较高。\n快速落地：不管是 Vue UI 这种全局 CLI 的形式，还是 Umi UI 这种跟随项目的 NPM 包形式，都有一个比较大的问题是版本碎片化，我们整体 Node 的生态社区，NPM 包版本的不稳定性，对同学来说是认知比较强烈的一个事，功能迭代后，让大家进行 NPM 包版本升级难度会比较大，而且同学会比较慌，而 VSCode IDE 侧的进行发布，它是可以进行动态版本下发的，保证我们大范围落地，这一部分的兼容成本就由我们框架团队内部消化，做一些兼容。\n\n3.2 提效率\n\n提效率的一个例子是 “一分钟内部署”，这里主要解决我们高频的测试环境部署流程冗长问题。2021 年我们 Rome ⾮线上环境发布⼤概 10W 次 / 年，核⼼流程 369s / 次，如下图所示：\n\n通过我们的开发辅助工具，可以做到页面粒度的 CSR/SSR 发布，同学不需要关注跨平台之间有哪些工作量，我们会在背后同步并且流转 DevOps 节点，把原本我们要在研发流程中，企业内要做质量分析、规范卡控等部分做异步，然后异步进行触发，整体的链路就可以保证一分钟内完成部署。\n\n3.3 提质量\n\nIDE 可以做开发时质量检测，开发时会有历史的线上故障提醒。\n\n哪些问题我们之前已经出过很多次线上故障了，在开发时就会实时检测代码并提示这部分应该进行修复，否则后续可能会出现线上故障。\n\n它和 ESLint 的区别：\n\n首先是我们可以做规则的动态下发，研发规则配置，做管理中心化；\n内容上我们也会支持故障包版本，比如业界之前出过几次故障包问题，包括 core-js、内部组件库故障版本等；\n我们也会把历史业务线上故障分析出来并提示，点击链接就可以跳转详细查看之前具体是出了哪次线上问题，是由什么引起的。\n\n3.4 平台流转\n\n我们内部研发时不仅要开发，还有设计稿，包括 DevOps 平台的流转等，也会上下游联动，保证设计可以快速到 DevOps 平台，DevOps 平台可以一键打开本地 VS Code，记忆历史的仓库和分支并快速打开，也可以快速流转到设计平台，开发完返回剩余流程。\n\n3.5 文档提示\n\n开发时也会给到大家文档提示，只要研发人员托管到到店内部的知识库，就可以在开发时，自动进行相关文档的匹配和跳转提示。\n\n3.6 CloudIDE 对接\n\n得益于业界 IDE 标准的高度统一，一般 CloudIDE 仅需一次开发，即可本地、云端 IDE 自动安装开发辅助，能力完全一致。\n\n4 框架度量和业务实践\n\n上文中从接入公司基建、编译提速、框架大版本升级等方面提到了很多工程能力，过程中也建设了升级、迁移工具，而作为企业内框架，也有一些问题和指标需要考虑。\n\n4.1 框架度量 - 核心问题\n\n问题和指标\n\n在业务规模不断扩大、能力落地覆盖范围及用户数量都在不断增加的背景下，框架现状衡量、框架如何发展等问题逐渐显现：\n\nRome 的用户群体从纯 B 端业务到覆盖 B&C 端业务，业务节点已覆盖 4 个事业群、15 个事业部；对比 2020 年，项目规模已经扩大了 11 倍，公司内共落地 1000 + 项目。\n\n如何判定标杆业务、框架能力支撑效果，如何持续快速统计用户规模并做运营？\n\n框架价值 = 覆盖范围 * 功能价值。能力开发完我们需要尽可能清晰化并提升落地覆盖，在统计运营上可以通过 CI/CD 平台发布过程数据、Git 仓库扫描数据、框架运行时数据打点、IDE 使用数据来做框架能力和用户规模数据采集；同时结合标杆团队、标杆业务以组织维度框架生态能力最大提效数据，并对能力验证和优化有正反馈，促进能力高质量、更大规模落地\n\n哪些生态能力使用频率高，哪些能力使用频率低，哪些是安装在项目中但没有实际使用的？\n\n面对已建设的生态能力，如果纯从能力开发视角出发、缺少有效的衡量体系，会导致有限的人力会被分摊到非常庞大的工程体系内：（1）已有的好能力因为没有度量数据，感知弱，在人员变动等客观背景下没有得到持续足够的推广导致好能力没有获得足够的落地，导致收益低于预期。（2）初期能力建设完后产生持续的维护成本，对部分使用率低的能力无法感知，未及时做出关键判断，产生一部分人力浪费。（3）对存在优化空间的能力，由于未及时拿到使用数据，不了解落地问题，能力未得到持续验证和优化而导致收益不达预期。\n\n框架如何从产品角度量化价值？\n\n从人力节省、质量、缩短交付周期等方面建立有效的产品价值衡量体系，对齐框架年度输出指标并持续运营。\n\n下个阶段框架发展方向和重点是什么？\n\n不变的是要深入结合业务架构重点支撑阶段核心业务场景，紧跟社区做一些先进能力，同时需要通过数据来辅助判断阶段重心是提升框架核心能力覆盖率，还是要建设新能力。\n\n框架面向用户群体的客服成本是多少？如何降低？\n\n收集客服数据，沉淀高质量的 FAQ，保证增量问题不被重复客服，引入 AIGC 智能客服等方式降低增量能力客服成本。\n\n4.2 框架度量 - 全景建设\n\n解决思路\n\n统一定义并梳理指标来源，结合上下游平台提取数据、对缺失数据采集并持久化，通过快照等形式完成数据层聚合和差异抹平；中间层会按照定义的大盘指标对数据进行统计拟合校准，包括项目覆盖率、核心能力使用率、接入率、研发环节效率等等；上层会按不同的业务部门、时间区段等提供可切换的覆盖、效能、质量等数据看板。\n\n数据来源层：\n\n研发过程数据持久化：覆盖研发全流程（工程框架内、插件内、CI 平台流水线节点内）\n生态产品数据埋点：Rome Works 质量检测功能埋点等\nTalos、Fedo 平台数据库\n\n数据统计层：\n\n多平台数据 Database\n关键信息月度快照：支撑多维度看板的数据二次处理\n\n大盘指标层：\n\nRome 大盘指标查询服务：核心包含输入输出指标的查询服务\nFEDO 工程化大盘指标查询服务：框架接入率、平均编译构建效率等指标\n\n交付（各模块的数据看板）：\n\n月度数据趋势对比\n可切换到餐 / 到店等组织维度的数据查询展示\n技术栈等细粒度分类的数据查询展示\n\n运营模块：\n\n数据：覆盖率指标的项目列表查询服务；X1 节点粒度的数据统计服务\n呈现：大盘中增加标杆团队数据图表；增加 X1 节点的团队预期收益统计查询服务\n4.3 业务实践\n\n整体 Rome 业务实践如下图所示：截止 2022 年，落地了 1400 多个工程项目，资产库的数量有 100 多个，到店多数 Web 项目是用 Rome 开发，大家打开美团 App，可以看到到店业务 H5 页面都是通过 Rome 进行开发的，有 B 端系统、C 端的 H5，还有我们美团内部的 React Native。\n\n5 总结及工程框架趋势思考\n\n工程框架不止于 CLI。要从需求交付的视角做框架，整个工程链路可能涉及到应用创建、应用认证、开发基建配置、依赖安装、编码、Mock、调试，包括提交 Lint、还有 Git、Install、Int、Build、Upload、灰度、Publish 等等。\n\n我们要核心关注高频的、规模化、价值比较高的环节：\n\n依赖安装环节，因为本地开发和线上部署都会使用到，如果这部分是当前核心的耗时点（发布总 300s，依赖安装 100s），下个阶段构建就可以先放一放，把依赖环节的优化提上日程；\n开发编译环节，很多时候开发环节我们提升了 50 秒，看起来好像没有那么高，但我们团队如果是 1000 人有几千个项目，调试编译频次是很高的，全量落地后规模化效果就会非常强；\n发布链路层面：交付链路中涉及整个研发的全部上下游，单纯做单点的工程优化（如本地构建优化）效果不一定好，可能到后来只是在卷几秒的差异。部署阶段依赖、构建缓存复用可能可以直接降低 10min / 次；另外部署时流水线节点异步触发、测试环境聚焦提速快速看到效果，线上再补充管理所需的质量分析、卡控等\n链路流转环节：工程框架和上游的设计协作平台怎么流转、下游如何和部署平台联动，他们的耗时可能也比较高，这部分也需要关注，因为对使用我们这套工具的同学来说，不关注这个能力是谁提供，而是我们在开发需求的时候，他明显会感觉到你们整体提供的这套产品，我用起来很难受，可能不一定是工程框架部分，单体都不错，一起用就是不够顺滑。\n\n未来趋势（个人阶段视角）：\n\n第一方面是工程框架要做开发链路的深度整合，可以类比业界的 Vercel，我们企业内也可以类比这些成熟的像 SaaS 产品去做；\n第二方面是此前提到的 Rust 基建、构建切换 Rspack/Vite、辅助开发工具，保证生态完善，公司内可以平滑升级；\nAI 时代：编码 Copilot；如何结合公司内 AI 基建把历史业务模板、组件库、资产库做提示生成是一个可以结合的点（框架有相对严格的资产生产结构规范，可以提升下游编码时资产消费准确率）；通过知识库问答降低技术项目的客服成本等。","str":"首先介绍一下业务的背景，这里主要从 3 个维度展开。第一个维度是组织维度，在立项之初，恰逢美团的多个事业群合并，因前端规模比较大，横向的流动协同比较多（需要跨部门支持需求，进行跨系统协作等等）。此外，美团到店事业群新人比例比较高，校招和新员工比例很高，我们会帮助新同学快速融入团队，需要完成一些较为基础的开发工作。"}],"note":"","style":"0","tags":[],"text":"首先介绍一下业务的背景，这里主要从 3 个维度展开。第一个维度是组织维度，在立项之初，恰逢美团的多个事业群合并，因前端规模比较大，横向的流动协同比较多（需要跨部门支持需求，进行跨系统协作等等）。此外，美团到店事业群新人比例比较高，校招和新员工比例很高，我们会帮助新同学快速融入团队，需要完成一些较为基础的开发工作。","type":"paragraph"},{"annote":"<sr-annote data-id=\"1739786170939\">第二维度是业务维度，美团到店业务迭代频次比较高，基础工程框架不仅要保证交付速度快，同时还对质量有很高的要求。</sr-annote>","color":1,"html":"第二维度是业务维度，美团到店业务迭代频次比较高，基础工程框架不仅要保证交付速度快，同时还对质量有很高的要求。","id":1739786170939,"items":[{"root":"1 背景介绍\n1.1 业务背景\n\n首先介绍一下业务的背景，这里主要从 3 个维度展开。第一个维度是组织维度，在立项之初，恰逢美团的多个事业群合并，因前端规模比较大，横向的流动协同比较多（需要跨部门支持需求，进行跨系统协作等等）。此外，美团到店事业群新人比例比较高，校招和新员工比例很高，我们会帮助新同学快速融入团队，需要完成一些较为基础的开发工作。\n\n第二维度是业务维度，美团到店业务迭代频次比较高，基础工程框架不仅要保证交付速度快，同时还对质量有很高的要求。\n\n第三个维度是系统维度，因业务周期比较长，到店还存在大量的存量系统，需要考虑迁移升级和重构等问题，同时会有频繁的系统交接。\n\n1.2 技术背景\n\n在 Rome 整体立项时，我们已经准备好了相关的基础设施，包括发布系统的收敛、基础架构，统一为基于 S3（美团内部存储服务）加动静分离的技术架构，但是上层开发框架、组件类库种类繁多且开发方式不统一。存在问题包括：整个团队中人数比较多，学习交接、建设维护成本相对较高，而整体开发的效率比较低，跨团队之间的工程能力也很难进行复用等等。\n\n建设之初，我们基于纯静态 S3（美团内部存储服务）架构进行前端框架的建设，这源于我们早期大量基于 Node.js 的前后端一体架构存在一些问题：首先，事业群早期以中后台场景业务为主，对页面的秒开、SEO 的诉求比较低；其次，当时 Node.js 生态基建还没有那么完善，前端同学需要做动态扩缩容、峰值流量处理等操作，整体的业务风险比较高。同时还存在机器成本高、开发人员能力要求高、招聘难度大等问题。\n\n因此，在整体的建设思路和路径上，我们不会建设类 Egg.js 这样的前后端一体的框架；同时因为我们的框架层要解决研发流程不规范、交付质量不高等问题，也需要联动上下游的设计研发、CI/CD 等系统形成一体的开发工程平台，而不只是做 CLI 工具。​\n\n2 工程生态、演变路径和规模化升级\n2.1 工程生态\n2.1.1 降学习成本\n\n框架约束\n\n根据前文所述，我们一开始要解决的核心问题是学习成本，因此我们会做框架约束。\n\n第一是工程目录，我们在真正开发的时候，比如 UI 框架是使用 Vue 还是 React，UI 框架本身的语法成本没有那么高，但是当拿到一个需求时，应该在哪儿改，在哪个位置梳理逻辑，这个部分的成本比较高，因为很多时候不同技术栈的业务组织方式不一样，会带来比较高的理解成本和心智负担。\n第二个是大团队下工程能力碎片化，团队间水平比较参差，带来的问题是团队之间难以做好、做深工程能力，典型的如 SSR、构建优化等很难跨团队间复用，一直在重复造低门槛的 “轮子”。\n第三是业务工程的配置比较冗长，带来的问题是工程配置比较耗时，很多时候存量系统不敢升级，因为不知道这么长的配置加了一些东西，是否会出现比较严重的后果。\n最后是公司基建落地，业务对接过程中非标的基建落地实践版本非常多；在公司开发和自己写小型项目有很大的差异，在公司内要解决监控、流量、部署对接等问题，需要做周期升级，而这部分如果不进行整体收敛，整体规模化升级成本就会比较高。\n\nRome 在这一侧的解法：\n\n统一目录认知：如下图右侧所示，在开发时，对于部门或者团队级别认可的规范，当技术同学不遵守这套规范的时候，在代码提交时会有卡控，当然也可以通过 TL 审批跳过。\n收敛依赖选型和版本：在业务开发时，从头进行工程框架选型的上手门槛比较高，因此在公司内将版本和选型进行统一收敛，保证大家可以专注业务。\n统一跨技术栈工程能力和开闭配置：保证大家拿到工程后，不管是哪个技术栈都可以以一个相对一致的认知进行开发，无需关注构建、工程能力配置等。\n收敛基础开发规范：默认在拿到一个项目时，如果没有特别定制化的团队规范，基本上可以不用关注像 ESLint、Prettier、EditorConfig、部署对接等开发规范。\n\n跨技术栈开发认知一致\n\n在实际迭代的时候，到店内部一开始整体收敛到 Vue 技术栈，后来因团队合并，又有了 React 技术栈。我们采取的第一个策略是把 Vue 和 React 整体目录保持大家认知的一致，如下图所示。而在真正开发的时候，比如配置子目录 SRC 下，当一个同学一开始是偏 Vue，但开发 React 需求的时候，即使团队和业务有一些变动，TA 也可以准确拿到项目快速启动，接下来摆在 TA 面前的可能就是特定技术栈下，对于对应的 API 理解是否深入的问题，这部分同学自己就可以解决。\n\n第二是我们会保证使用 Rome 开发框架的整体开发调试流程和体验一致。如下图，在 Vue 或 React 框架中同学专注业务开发就行，基本上可以不用关注工程框架的相关事情。比如 Vite 在 Vue 或在 React 指令和表现是一致的，同学可以通过一行命令直接开始调试对应项目并进行开发。\n\n第三，我们的整体工程能力会进行高度对齐，如上图所示。我们生态下 80% 的插件在 Vue 和 React 下，用法和表现完全一致，大家基本上不用关心这部分，当真正用到某部分能力，详细看对应的文档就行。\n\n基础基建对接\n\n工程能力：一行代码引入公司基建（CDN 容灾）\n\n当前端同学处理如 CDN 厂商故障等问题时，需要自行查找、学习和使用公司 / 开源的基建能力，解决成本较高，但在 Rome 开发过程中可以通过开闭式的配置一键 CDN 容灾生效，如下图，接入后会自动进行静态资源的降级重试，当业务出现故障时（即下图峰值部分），它会有一个明显的资源加载重试，这就是降级 CDN 在生效；对业务同学来说只需配置框架需要哪个能力就行，其详细配置如何注入大家并不需要太关注。\n\n工程能力：一行代码引入公司基建（告警监控）\n\n业务开发以外的像告警监控，对同学来说，可能也要理解很多东西。但是在企业内开发，我们希望同学可以专注业务开发，像线上告警、日志链路等出现问题，可以不用配置对应平台，我们用对应的项目 Key 就可以查到对应的错误总量、错误调用链路等。\n\n工程能力：一行代码引入公司基建（水印）\n\n我们也可以一键完成对应的水印接入，同时开发配置时有对应的配置项智能提示。各种开闭能力的属性命名语义可能没那么清晰，可以通过如 VSCode 中的智能提示一键跳转到对应能力的文档地址，查看它的实现原理以及使用方法。\n\n2.1.2 降建设成本\n\n之前组织下建设的很多能力是偏小蘑菇的形式，“小蘑菇” 的语义是整体工程能力的生命周期比较短，会进行频繁的重复建设。但我们希望以一个大树的形式建设，“大树” 的语义是我们可以通过跨团队、横向方式进行高质量的设计和建设，通过评审委员会把控整体的方案建设，把方案拆分到几个阶段，横向团队共建，共同完成核心能力，比如 Serverless、SSR 等。\n\n工程能力生态\n\n如果大家对工程框架的建设能力感兴趣，可以参考我们内部的效率、质量、体验和领域实践进行对应分类的建设。我们通过一线调研、能力盘点的方式建设了 31 个，在企业内进行建设时，有很多能力一开始可能本身成本就很高，比如统一基建接入等，大概需要 30 人力 / 个，而实际业务需求开发一共只有几天时间，很难说服产品运营单独抽出大量人力开发前端基建能力，这部分我们就以共建 + 横向评审的方式，保证业务团队可以摊平整体的建设成本。\n\n典型共建案例\n\nServerless SSR 能力，在休娱频道的密室和优选团长端的订单管理流量分别是百万级别和千万级别，整体秒开可以做到 90% 左右。\n\n整个过程是 Rome + Arche 团队共建并形成到店标准，业务统一落地，过程中有超过 3 个业务团队参与了部分能力建设。\n\n这里简要介绍下 Serverless 的方案流程：\n\n首先是访问自动降级，用户通过负载均衡命中对应的网关，命中对应的 Serverless 的 Node Runtime，这里会有对应页面粒度的 SSR 渲染函数，当用户访问服务器返回 5.x.x 状态码或超时时会自动降级到 CSR 资源（SPA 页面）；即使出现了极端情况，比如 SSR 服务异常，我们也可以保证整体页面可用，差别主要在 SSR 场景秒开~ 90%，CSR 场景秒开~ 30%，兼顾了性能和稳定性的优势。\n\n其次是支持页面粒度接入，在大型 MPA 项目开发中，可以把某个页面单独设置为 SSR，不必要求整个项目都是 SSR 工程，构建时默认会上传 CSR 端构建资源，而针对 SSR 页面会额外进行另外一份资源的构建，在部署 SSR 端、CSR 端资源时分别部署，用户访问页面链接时，对应命中 Server 端或者 CDN 资源。\n\n支持自动扩缩容和流失渲染，对接公司 Nest 基建，支持业务无感知的峰值流量自动扩缩容；同时长列表页面也可以通过流式渲染来降低 TTFB 时间。\n\n2.1.3 提研发效率\n\n编译提速主要分两个方向进行业务落地，分别是 Webpack 体系和 Vite 体系。\n\n编译提速 - Webpack 体系优化\n\n首先是 Webpack 体系，因为我们整体的技术栈分 Vue 和 React 两部分，我们会抽一部分基础的跨技术栈的 Webpack 配置，上层有不同团队分别维护 Vue 技术栈和 React 技术栈的特有配置，中间我们会进行专项优化，包括像 Webpack5 的开发优化、编译压缩器、SWC 和 esbuild 等。\n\n另外，我们做工具时，不仅要考虑我们本身构建工具比如 Webpack，也要考虑如何和我们的 CI/CD 平台结合，如缓存复用结合依赖安装、构建工具做，在很多需求场景下，可以获得耗时的 10 倍提升，CI/CD 侧我们也支持分粒度的构建，比如大型 MPA 做页面粒度的构建等。\n\n编译提速 - 开发时⼀键 Vite\n\nVite 主要是我们在开发时使用的，Webpack 是构建时使用，除了少部分增量项目的开发和构建都走 Vite，大多数存量项目都是开发使用 Vite 保证效果，构建阶段使用 Webpack。\n\n整体流程：首先是启动，通过 Vite，比如 createServer 拉起页面，抹平 Webpack 差异；环境变量的抹平，保证客户端 Webpack 存量项目，将 Webpack 客户端的环境变量注入到 Vite 端；对配置文件，大家原本在 rome.config.js 中的 Webpack 配置也可以一键转换为对应的 Vite 配置；在下层我们会做内置的技术栈插件，包括 Vue2/Vue3 SFC+JSX，包括 react 的内置插件等；比较繁琐的是生态兼容，在公司内这部分的工作量比较大，像我们的一些 SaaS 的静态资源的一些路径等，包括之前我们存量 Webpack 这么多年积累下来的公司基建，我们如何保证它能被注入到 Vite 端，包括组件库按需引入，当一些模块比如 common.js 的模块如何指向对应的 esmodule 目录等，最后我们会屏蔽一些认知成本，把 Vue、React 和公司依赖包内置到 Vite 预编译内容中。\n\n依赖提速\n\n主要分两部分，分别是开发阶段和部署阶段。\n\n首先会做工程框架自身 Node.js 依赖的预构建，这部分核心解决的是 Node.js 端公司内外包资源的体积和递归依赖数量庞大的问题。依赖安装的一个核心耗时其实是递归依赖庞杂，需要逐个安装带来的耗时，比如需要安装 A，其实 A 依赖了 B、C、D、E、F、G，整体安装耗时就会非常久，同时研发同学对 Node.js 包体积、依赖数量一般不如面向浏览器端投放的 NPM 包那么敏感，这个时候预编译就很关键。\n\n其次是把整体的依赖环节包管理工具切换到 Pnpm。这部分经过调研（Pnpm、Yarn Berry、Yarn Berry with PNP 等），判断 Pnpm 是未来几年的核心趋势，同时可以稳定发展。内部建设了对应的包管理迁移工具，来服务企业内几百个、上千个项目。\n\n在部署阶段定制了 Rome 依赖安装 Docker 镜像，锁 Pnpm 版本和内置常用依赖、缓存复用，它的整体逻辑是上一次需求迭代和下一次需求迭代可能依赖包的变化没有那么多。我们在第二次发布的时候，如果能够尝试命中上一次的 node_modules 里的一些包，那第二次安装耗时就会有一个比较大的提升。\n\n2.2 演变路径\n\n第一阶段的时候是强依赖 Webpack 和 Vue 技术栈的。直到 2020 年 - 2021 年，像社区 UI 框架有 Solid.js、Svelte、Vue3，构建工具有 Esbuild、Rspack、Vite 等且发展势头也比较好，面向这个大趋势，我们希望研发框架能够分层过渡，不再和具体的 UI 框架、构建工具绑定：\n\n首先最底层是差异机制，核心做插件的加载和调度；\n中间层是构建工具的 bundler ，这一层主要是分别结合 Webpack、esbuild、Rspack 等构建实现，支持大量项目；\n上层 Plugin、Preset 做框架拓展，各个业务能够基于场景定制自己的工程能力；\n顶层是品牌指令（bin），这部分主要是企业内合作时业务方希望保持自己的品牌名。\n\n示例如下图右侧，大多数情况下，我们有一个新的研发框架出来，只需要开发我们对应的语言特色的插件集合就行，底层插件机制 + 构建工具大家其实可以保持一致的。\n\n2.3 规模化升级\n\n框架价值 = 覆盖范围 * 功能价值。我们的共识是建设的能力需要落地才能产生价值，核心是帮助大家业务低成本地享受好用的能力。\n\n如下图所示，是某个业务的升级成本图，可以看到有 100 多个项目（成本均值~ 5pd / 项目），业务面临几百天的成本。对团队 Tech Lead 很难说服业务产品运营业务阶段投入升级。对一线同学来说，虽然能力很多、很棒，但是升级风险怎么处理：编译提升 10s，线上问题定位消耗一周。\n\n这部分的我们给两个解决方案。\n\n首先是提供迁移工具（非 Rome 到 Rome 项目）：\n\n巨石项目做存量迁移时，一般同学下意识反应是通过 Babel 做静态解析迁移，纯静态解析实测的迁移准确率在 40%，也就说一个项目 40% 的事情通过 Babel 可以搞定。\n第二阶段是基于构建工具的依赖引用关系去做迁移，通过构建工具启动时的模块工厂钩子去生成依赖引用树状结构然后做迁移，得到迁移准确率在 60% 左右。\n第三阶段是动态分析结合 AST 静态分析一起去做，在数据采集阶段通过构建工具的模块工厂钩子、Vue/React 实例化对象的路由 / 数据流配置去生成依赖图结构，通过 recast 等工具进行静态解析，不用 Babel 是因为 recast 有比较好保持样式的代码回写，文件内容增删改后写回原文件样式不乱，同学在 git diff 时需要 Code Review 代码就可以少一些，便于迁移影响范围判断。\n最后配备上基础公司基建做的一些依赖版本、体积、兼容性检测的质量工具，自动迁移了差不多有 100 多个项目，线上零 Case。\n\n另一个是做框架生态的大版本升级，假如一个季度推出来一个大版本，实际业务开发升级难度、成本就会比较大，业务可能也不敢升级，这时大版本升级就要提供关键流程自动迁移支持：\n\n当从 Yarn 到 Pnpm 的时候，会做幽灵依赖检测，自动向 Pnpm/src 的幽灵依赖配置等；\n也会做工程源码对比；\n构建配置，如 Webpack 前后框架无关的配置对比；\n产物变动升级前后 entry 数量是否不一样、产物有没有明显变大；\n兼容性检测，升级前是 ES 5 + 的代码，升级后到 ES 6 了，那也会出问题；\n线上验证等。\n\n公司内部的一些标杆团队都是由我们 Rome 团队升级 + 写报告，标杆业务由基建团队升级的话，ROI 可能会比较高，我们升级通过工具将我们的二次调整，整体成本可能会非常低，2~4h 就可以完成一个中等存量项目，再由核心业务同学验收、线上灰度跟进等。\n\n3 框架开发辅助\n\n我们 Rome 的工程开发辅助工具是基于 IDE 的。一般业界的开发辅助工具两种形式，下图是 Vue UI 和 VSCode 的拓展图示：\n\n一是通过 Web 实现，典型案例有 Vue UI/Umi UI/Angular Console 等；\n二是 VSCode 拓展，如 Ice Works 等。\n3.1 为什么要基于 IDE？\n\n理解：对于 Web 形式，业界以 Umi UI（更先进）和 Vue CLI 为主，虽然它们都是基于 Web 实现，但出发点不太一样，Vue UI 核心是做体验提升，支持工程创建 CLI 可视化，工程配置分析等；但是 UMI UI 要服务公司内部，更多的是做提效，支持一些组的资产插入、执行工程任务比如 build/lint 等。\n\n基于 Web 做开发辅助工具\n\n好处：跨 IDE 的成本比较低，前端同学覆盖会比较全，也可以解决 IDE 内功能交互 UI 受限问题。\n\n问题：VSCode 不开放这部分的 UI 侵入能力，一方面心智认知类 Umi UI 的趋势其实把开发时的用户心智切换到 Web 端。但我们研发时，其实要在 IDE 看代码、理解代码、写代码，当我们热更新起来以后，像构建、物料插入、Link 等其实比较低频，就会带来一个问题，我们研发时的心智很难切到对应的浏览器端。另一方面研发链路，长期来看，不管是业界还是美团内部的研发趋势中就一部分场景，后续肯定是用 Cloud IDE，但是我们工程框架的 Web 端这种开发辅助工具，如果通过 Web 形式实现，就很难融入到我们的研发链路中。\n\nIDE 侧方案确实存在 UI 交互受限、视窗面积、跨 IDE 开发成本等问题，因为同学在开发时，VSCode 的面积就这么大，每弹出一个 Webview 就会侵占用户的开发视窗，会比较难受。\n\n选型原因：\n\n用户频次：我们核心是企业内做提效，那就决定了绕不开 IDE 侧，而且 TA 是用户高频部分，而且我们在代码侧也可以复用 VSCode、IDE 侧的 LS API，交互受限通过用户引导是可以接受的。\n研发整合：我们后续可以在 Cloud IDE 整体比较成熟的时候，会把部分轻量迭代切换到 Cloud IDE 中进行研发，同时希望新模式下开发辅助工具可以继续产生价值。第二是可以在 IDE 侧整合上下游链路，保证用户心智可以一直停留在 IDE 内而不用跨平台操作，整体成本可能会比较高。\n快速落地：不管是 Vue UI 这种全局 CLI 的形式，还是 Umi UI 这种跟随项目的 NPM 包形式，都有一个比较大的问题是版本碎片化，我们整体 Node 的生态社区，NPM 包版本的不稳定性，对同学来说是认知比较强烈的一个事，功能迭代后，让大家进行 NPM 包版本升级难度会比较大，而且同学会比较慌，而 VSCode IDE 侧的进行发布，它是可以进行动态版本下发的，保证我们大范围落地，这一部分的兼容成本就由我们框架团队内部消化，做一些兼容。\n\n3.2 提效率\n\n提效率的一个例子是 “一分钟内部署”，这里主要解决我们高频的测试环境部署流程冗长问题。2021 年我们 Rome ⾮线上环境发布⼤概 10W 次 / 年，核⼼流程 369s / 次，如下图所示：\n\n通过我们的开发辅助工具，可以做到页面粒度的 CSR/SSR 发布，同学不需要关注跨平台之间有哪些工作量，我们会在背后同步并且流转 DevOps 节点，把原本我们要在研发流程中，企业内要做质量分析、规范卡控等部分做异步，然后异步进行触发，整体的链路就可以保证一分钟内完成部署。\n\n3.3 提质量\n\nIDE 可以做开发时质量检测，开发时会有历史的线上故障提醒。\n\n哪些问题我们之前已经出过很多次线上故障了，在开发时就会实时检测代码并提示这部分应该进行修复，否则后续可能会出现线上故障。\n\n它和 ESLint 的区别：\n\n首先是我们可以做规则的动态下发，研发规则配置，做管理中心化；\n内容上我们也会支持故障包版本，比如业界之前出过几次故障包问题，包括 core-js、内部组件库故障版本等；\n我们也会把历史业务线上故障分析出来并提示，点击链接就可以跳转详细查看之前具体是出了哪次线上问题，是由什么引起的。\n\n3.4 平台流转\n\n我们内部研发时不仅要开发，还有设计稿，包括 DevOps 平台的流转等，也会上下游联动，保证设计可以快速到 DevOps 平台，DevOps 平台可以一键打开本地 VS Code，记忆历史的仓库和分支并快速打开，也可以快速流转到设计平台，开发完返回剩余流程。\n\n3.5 文档提示\n\n开发时也会给到大家文档提示，只要研发人员托管到到店内部的知识库，就可以在开发时，自动进行相关文档的匹配和跳转提示。\n\n3.6 CloudIDE 对接\n\n得益于业界 IDE 标准的高度统一，一般 CloudIDE 仅需一次开发，即可本地、云端 IDE 自动安装开发辅助，能力完全一致。\n\n4 框架度量和业务实践\n\n上文中从接入公司基建、编译提速、框架大版本升级等方面提到了很多工程能力，过程中也建设了升级、迁移工具，而作为企业内框架，也有一些问题和指标需要考虑。\n\n4.1 框架度量 - 核心问题\n\n问题和指标\n\n在业务规模不断扩大、能力落地覆盖范围及用户数量都在不断增加的背景下，框架现状衡量、框架如何发展等问题逐渐显现：\n\nRome 的用户群体从纯 B 端业务到覆盖 B&C 端业务，业务节点已覆盖 4 个事业群、15 个事业部；对比 2020 年，项目规模已经扩大了 11 倍，公司内共落地 1000 + 项目。\n\n如何判定标杆业务、框架能力支撑效果，如何持续快速统计用户规模并做运营？\n\n框架价值 = 覆盖范围 * 功能价值。能力开发完我们需要尽可能清晰化并提升落地覆盖，在统计运营上可以通过 CI/CD 平台发布过程数据、Git 仓库扫描数据、框架运行时数据打点、IDE 使用数据来做框架能力和用户规模数据采集；同时结合标杆团队、标杆业务以组织维度框架生态能力最大提效数据，并对能力验证和优化有正反馈，促进能力高质量、更大规模落地\n\n哪些生态能力使用频率高，哪些能力使用频率低，哪些是安装在项目中但没有实际使用的？\n\n面对已建设的生态能力，如果纯从能力开发视角出发、缺少有效的衡量体系，会导致有限的人力会被分摊到非常庞大的工程体系内：（1）已有的好能力因为没有度量数据，感知弱，在人员变动等客观背景下没有得到持续足够的推广导致好能力没有获得足够的落地，导致收益低于预期。（2）初期能力建设完后产生持续的维护成本，对部分使用率低的能力无法感知，未及时做出关键判断，产生一部分人力浪费。（3）对存在优化空间的能力，由于未及时拿到使用数据，不了解落地问题，能力未得到持续验证和优化而导致收益不达预期。\n\n框架如何从产品角度量化价值？\n\n从人力节省、质量、缩短交付周期等方面建立有效的产品价值衡量体系，对齐框架年度输出指标并持续运营。\n\n下个阶段框架发展方向和重点是什么？\n\n不变的是要深入结合业务架构重点支撑阶段核心业务场景，紧跟社区做一些先进能力，同时需要通过数据来辅助判断阶段重心是提升框架核心能力覆盖率，还是要建设新能力。\n\n框架面向用户群体的客服成本是多少？如何降低？\n\n收集客服数据，沉淀高质量的 FAQ，保证增量问题不被重复客服，引入 AIGC 智能客服等方式降低增量能力客服成本。\n\n4.2 框架度量 - 全景建设\n\n解决思路\n\n统一定义并梳理指标来源，结合上下游平台提取数据、对缺失数据采集并持久化，通过快照等形式完成数据层聚合和差异抹平；中间层会按照定义的大盘指标对数据进行统计拟合校准，包括项目覆盖率、核心能力使用率、接入率、研发环节效率等等；上层会按不同的业务部门、时间区段等提供可切换的覆盖、效能、质量等数据看板。\n\n数据来源层：\n\n研发过程数据持久化：覆盖研发全流程（工程框架内、插件内、CI 平台流水线节点内）\n生态产品数据埋点：Rome Works 质量检测功能埋点等\nTalos、Fedo 平台数据库\n\n数据统计层：\n\n多平台数据 Database\n关键信息月度快照：支撑多维度看板的数据二次处理\n\n大盘指标层：\n\nRome 大盘指标查询服务：核心包含输入输出指标的查询服务\nFEDO 工程化大盘指标查询服务：框架接入率、平均编译构建效率等指标\n\n交付（各模块的数据看板）：\n\n月度数据趋势对比\n可切换到餐 / 到店等组织维度的数据查询展示\n技术栈等细粒度分类的数据查询展示\n\n运营模块：\n\n数据：覆盖率指标的项目列表查询服务；X1 节点粒度的数据统计服务\n呈现：大盘中增加标杆团队数据图表；增加 X1 节点的团队预期收益统计查询服务\n4.3 业务实践\n\n整体 Rome 业务实践如下图所示：截止 2022 年，落地了 1400 多个工程项目，资产库的数量有 100 多个，到店多数 Web 项目是用 Rome 开发，大家打开美团 App，可以看到到店业务 H5 页面都是通过 Rome 进行开发的，有 B 端系统、C 端的 H5，还有我们美团内部的 React Native。\n\n5 总结及工程框架趋势思考\n\n工程框架不止于 CLI。要从需求交付的视角做框架，整个工程链路可能涉及到应用创建、应用认证、开发基建配置、依赖安装、编码、Mock、调试，包括提交 Lint、还有 Git、Install、Int、Build、Upload、灰度、Publish 等等。\n\n我们要核心关注高频的、规模化、价值比较高的环节：\n\n依赖安装环节，因为本地开发和线上部署都会使用到，如果这部分是当前核心的耗时点（发布总 300s，依赖安装 100s），下个阶段构建就可以先放一放，把依赖环节的优化提上日程；\n开发编译环节，很多时候开发环节我们提升了 50 秒，看起来好像没有那么高，但我们团队如果是 1000 人有几千个项目，调试编译频次是很高的，全量落地后规模化效果就会非常强；\n发布链路层面：交付链路中涉及整个研发的全部上下游，单纯做单点的工程优化（如本地构建优化）效果不一定好，可能到后来只是在卷几秒的差异。部署阶段依赖、构建缓存复用可能可以直接降低 10min / 次；另外部署时流水线节点异步触发、测试环境聚焦提速快速看到效果，线上再补充管理所需的质量分析、卡控等\n链路流转环节：工程框架和上游的设计协作平台怎么流转、下游如何和部署平台联动，他们的耗时可能也比较高，这部分也需要关注，因为对使用我们这套工具的同学来说，不关注这个能力是谁提供，而是我们在开发需求的时候，他明显会感觉到你们整体提供的这套产品，我用起来很难受，可能不一定是工程框架部分，单体都不错，一起用就是不够顺滑。\n\n未来趋势（个人阶段视角）：\n\n第一方面是工程框架要做开发链路的深度整合，可以类比业界的 Vercel，我们企业内也可以类比这些成熟的像 SaaS 产品去做；\n第二方面是此前提到的 Rust 基建、构建切换 Rspack/Vite、辅助开发工具，保证生态完善，公司内可以平滑升级；\nAI 时代：编码 Copilot；如何结合公司内 AI 基建把历史业务模板、组件库、资产库做提示生成是一个可以结合的点（框架有相对严格的资产生产结构规范，可以提升下游编码时资产消费准确率）；通过知识库问答降低技术项目的客服成本等。","str":"第二维度是业务维度，美团到店业务迭代频次比较高，基础工程框架不仅要保证交付速度快，同时还对质量有很高的要求。"}],"note":"","style":"0","tags":[],"text":"第二维度是业务维度，美团到店业务迭代频次比较高，基础工程框架不仅要保证交付速度快，同时还对质量有很高的要求。","type":"paragraph"}],"create":"2025年02月17日 17:54:40","desc":"本文整理自美团技术沙龙第 76 期《大前端研发协同效能提升与实践》，为大家介绍了美团到店前端研发框架 Rome 实践和演进趋势。","favicon":"https://awps-assets.meituan.net/mit/blog/v20190629/asset/icon/android-icon-192x192.png?v=Whistle&t=20181017-1r","idx":2,"img":"https://p0.meituan.net/travelcube/d09601a0be6cce128a25e2ac8256a71881949.png","note":"","tags":[],"title":"美团前端研发框架 Rome 实践和演进趋势 - 美团技术团队","url":"https://tech.meituan.com/2023/08/03/meituan-rome-practice.html"},{"annotations":[{"annote":"<sr-annote data-id=\"1739766046174\">第二层，移动端开发从一开始就有原生开发和 H5/JS 开发（PhoneGap 发布于 2009 年），但 JS 成为主流，始于 2015 年，Google 把 V8 适配到了 Android 上。这里就不是 JS 行不行，而是 V8 很行。 终端硬件的计算资源始终是有限的，V8 通过解释器层面 JIT/AOT 编译技术 大幅提升了 JS 的运行效率，使得 JS 铺开成为可能。但近几年 JS 引擎不再有大幅度迭代，手机硬件算力的升级速度也明显放缓，这就导致了终端上 软硬件计算容量的提升速度跟不上业务复杂度的提升速度。这才是当下 JS 技术栈面临的问题，也是为什么有如此多 JS 框架在卷性能的原因。</sr-annote>","color":1,"html":"第二层，移动端开发从一开始就有原生开发和 H5/JS 开发（PhoneGap 发布于 2009 年），但 JS 成为主流，始于 2015 年，Google 把 V8 适配到了 Android 上。这里就不是 JS 行不行，而是 V8 很行。 终端硬件的计算资源始终是有限的，V8 通过解释器层面 JIT/AOT 编译技术 大幅提升了 JS 的运行效率，使得 JS 铺开成为可能。但近几年 JS 引擎不再有大幅度迭代，手机硬件算力的升级速度也明显放缓，这就导致了终端上 软硬件计算容量的提升速度跟不上业务复杂度的提升速度。这才是当下 JS 技术栈面临的问题，也是为什么有如此多 JS 框架在卷性能的原因。","id":1739766046174,"items":[{"root":"本文整理自美团技术沙龙第 83 期《前端新动向》（B 站视频）。长久以来，容器要实现动态化和双端复用，难免要牺牲掉一些性能。有没有办法让动态化容器的性能尽可能接近原生？美团金服大前端团队给出了一种解决方案，尝试突破动态化容器的天花板。\n\n1 动态化容器的天花板\n\n自 2015 年 React Native推出至今 9 年时间，各类容器（动态化容器简称，下同）方案已经成为业界前端的普遍选择。业界有微信（小程序）、抖音（Lynx）、拼多多（Lego）、支付宝（Nebula/BirdNest）、京东（Taro-Native）等。美团也有 MRN、MMP/MSC 等容器。可以说容器是前端工程的关键基石，也是绕不开的话题。\n\n过去我们做动态化改造主要为了解决以下问题：\n\n降研发成本：通过容器将多端合一，避免一个需求在每个端重复开发，以改善研发成本结构。随着 HarmonyOS NEXT 的推广，这个优势将变得更大。\n增部署效率：通过动态发布避开 App 集中集成，使得业务在移动端上可以独立部署和发布、实现 211 迭代，提升业务迭代面客效率。\n\n然而凡事有利必有弊，有用必有费。动态化容器在解决上述问题的同时也带来以下问题：\n\n降低页面成功：动态化容器引入了动态部署、解释器等更多的环节。在增加整体复杂度的同时，更多的环节也带来了更多的错误和计算开销，具体体现在页面白屏和页面加载耗时增加上。\n牺牲用户体验：动态化容器需要更多的硬件算力，相同的业务复杂度下，容器化页面相较原生页面更慢、更卡、不流畅，这在下沉市场设备上更为突出，卡顿甚至成为卡死。\n\n动态化容器的绝对天花板是原生应用，目前事实天花板是 React Native/WebView。\n\n定性地看前端容器天花板的问题，这里引述我们美团容器界的一位前辈的理论：性能、效能、动态化是动态化容器的不可能三角（下图左）。现有的通用的容器方案都是在这三个维度做 “三选二”。\n\n定量地看动态化容器，下图（右）展示了一个 3000 个相同视图节点的简单 Benchmark 页面。没有额外逻辑，也没有网络请求，以 React Native 为例，在同一台设备上，在 React Native 上做一个这个页面，动态化页面加载耗时大约是 Native 原生页面的 5 倍。\n\n这个测试结果不一定容易接受，但是很好理解。在相同的业务复杂度下，动态化容器为了实现动态化，引入了一个逻辑解释器，增加了解释执行和与解释器通信的额外计算开销，这就是动态化页面性能表现差的主要原因。既享受动态化的好处，又不牺牲用户性能体验，只在现有的方案上做选择是不可能达到的。不过金服大前端团队在这个问题上却是取得了一些实质性进展。\n\n先说一下我们目前取得的成果：美团金服前端团队做了一个新的容器 Recce，然后在同样的测试页中，我们将执行业务逻辑部分的速度提升了 8 倍，整体的页面加载速度提升了一倍。而在实际的业务中，页面加载速度也实现了 3 倍的提升，在兼顾动态化和效能的前提下，实现了性能的大跨步提升，性能表现接近 Native 原生。下文将重点介绍 Recce 具体是怎么实现的，希望能够给大家提供一些帮助或借鉴。\n\n2 容器分类及前期思考\n\n首先，当我们计划做一个容器之前，需要先对现有容器建立基本的认识。\n\n前端容器最重要部分之一在于绘制图形界面以完成人机交互，现有主要容器方案按照绘制方式可归纳为下列三类：\n\n第一类可以称为基于 Web 的方案，这类的共同特点就是调用 WebView， 通过 JavaScript 和 CSS 去绘制页面，然后通过 Web 提供的接口去和宿主通信。\n第二类称之为 “自己绘制”，它会调用更底层的 OpenGL 等图形的绘制框架，同时也会有自己的一套方案和语言去标记和编写自己的这些页面。\n第三类则是去调用系统的 UI 框架，就是基于平台提供的 UI 框架去进行绘制，和第二种的区别是，会有一层抹平平台差异的平台抽象层。\n\n对现有的容器再做进一步的结构分解，方便我们对不同容器方案之间做对比，这里拆分为四层：\n\n第一层，也就是最上层的 UI 框架，跟我们直接平时写的代码相关，它会直接决定业务代码的样子。\n第二层，其作用就像它的名字一样，也就是运行时支持，为运行 UI 框架提供支持，在这里会有解释器或者是标准库之类的东西。\n第三层，会对不同平台去做一层不同的抽象，比如像 RN 会对视图操作之类的统一为相同概念和统一接口。\n第四层，渲染层，对应着不同分类选择的各自的渲染的方式。\n\n那么下面，我们就会基于上图对现有容器这个分类和结构为认知基础，结合之前影响容器性能表现最大的因素在于逻辑解释器执行效率和逻辑解释器通信效率这个认知，再去考虑实现一个满足性能、安全、动态化的容器方案该怎么做。\n\n3 Recce 的选型与搭建\n\n所以如何选择 UI 框架、运行时支持、平台抽象层、渲染层 来实现一个高性能、安全的动态化容器呢。首先把渲染层放到最后，因为渲染层作为最底层并不影响上层选型。然后我们首先讨论运行时支持怎么选，准确的说是解释器和编程语言怎么选，因为编程语言会影响上层 UI 框架，而解释器也影响到平台抽象层中的通信部分，所以接下来的讨论顺序是：解释器 & 编程语言、UI 框架、渲染层以及整体架构这四部分。\n\n3.1 解释器 & 编程语言的选择\n\n解释器以 Wasm 为主， JavaScript 为辅\n\n前文也提到，我们期望能获得一个既能满足性能，又安全，同时还可以动态化这样的一个方案。既然必须动态化，就必须有逻辑解释器 [1]，问题就变成了怎样选一个性能好且安全的解释器，并且成本在可接受范围。现成的解释器还是不少的，前端范畴有 V8、JavaScriptCore、QuickJS 等 JavaScript 解释器，有符合 WebAssembly（后简称 Wasm） 规范的 Wasmer、WasmEdge[2]；大家日常工作中会接触到的 Ruby、Python；还有游戏行业用的比较多的 Lua、C#[3]。\n\n首先可以排除掉 Ruby 和 Python 这两个语言和解释器，无论是性能还是生态都不如 JavaScript。Lua 和 C# 也可以排除，主要是游戏生态和前端差距太远，晚饭想吃炸鸡，中午才开始孵蛋显然就来不及了。在 JavaScript 解释器和 Wasm 解释器两个大范围里，Wasm 解释器在性能和安全上较 JavaScript 解释器有决定性优势，生态上较 JavaScript 略差，但都在 W3C 标准范围内，一样可以运行在 H5 和小程序里，“晚饭想吃炸鸡，中午开始杀鸡还是来得及的”。\n\n所以在解释器的选型上，就确定了 Wasm 解释器为主，JavaScript 为辅的基本策略。而 Wasm 解释器具体选择是 Wasm3，原因有两方面：第一，这是在不支持 JIT[4] 下最快的 Wasm 解释器；第二，对包大小占用非常少。\n\n编程语言选择\n\n在确定了 Wasm 解释器之后，编程语言的选择就变成了，在支持 Wasm 的语言里选择性能、安全和成本最优的。理论上可以编译成 WebAssembly 执行的语言非常之多，但真正成熟到可以上生产环境的只有 C、C++、Rust、Go 这四种 [5]。首先可以先排除掉 C，虽然性能好但是不支持高级抽象只适合用在嵌入式等极端场景，不适合用来前端写业务。然后可以排除 C++，性能表现上 C++ 和 Rust 不相上下都好过 Go，但是错误管理和内存安全上输 Rust 一筹，并且 C++ 在前端业务层也是生态基本为零，不像 Rust 在前端生态发展迅猛。最后可以排除 Go，在性能表现上、类型系统设计、错误管理、还有前端生态上都输 Rust 一筹。\n\n综上所述，在运行时我们就选择了 Rust 和 Wasm3，JavaScript 和 QuickJS 后面再进行介绍。\n\n3.2 UI 框架\n\n用 JavaScript/QuickJS 的部分可以复用 Vue 或者 React，这些先不提。用 Rust 则必须要为运行时的上层设计一套 UI 框架，确定应该怎么样编写页面。这项工作的挑战在于，它并不像 JavaScript 生态，有多年的积累可供参考或复用，比如经典的 React 和 Vue。当然好处在于也没有历史包袱，所以必须要结合 Rust 语言的特点才能更好地完成这个任务。\n\nUI 框架大抵需要做到三点：（1）提供声明式 DSL 方便前端研发描述界面；（2）提供组件封装和状态管理能力完成业务逻辑和用户交互的衔接；（3）性能卓越。其中（1）和（2）在 JavaScript 生态中已经都有实现，（3）则未必，否则也不会有几十种 Web UI 框架并存这种局面。这个问题难在如何用 Rust 这门强类型纯静态语言去实现 JavaScript 弱类型动态语言实现的功能，并且要维持 Rust 零开销抽象的优势。\n\n为了解决这个问题，我们参考了 GitHub 上开源的各种框架。一方面参考了 Dioxus 的 DSL 设计和 UI 封装，另一方面也保持了 Rust-Dominator 观察者模式订阅变更的更新效率，我们将这两个优点合并到一起，就得到了 Recce UI，就其特点，没有 Diff，也没有 VDOM，跟 SolidJS 一样实现真正的订阅，我们也尽可能地去保证可以高效地构建 UI。下图中的表格对应的就是一些 Web 项目下的性能对比，这个也并没有直接对应到具体实践的容器上，因为我们也做了一个类似 Native 渲染的东西，所以这个表格对我们来说具备很好的参考价值，至少可以看到 Dominator 的更新效率还是很不错的。\n\n3.3 渲染层\n\n渲染层的选择则相对简单，如前述归纳实现一个渲染层大致三种方式：复用 WebView、自建渲染绘制器、调用系统 UI 框架。\n\n复用 WebView：如果追求高性能，这条路就不通，复用 WebView 意味着渲染指令 / 视图树 要用低效的方式以 WebView 的 JsCore 为跳板再去驱动 WebCore 做渲染。这和高性能就南辕北辙，还不如纯 H5 性能表现好。\n自建渲染绘制器：这条路技术上是行得通的，但目前走不得：第一，从代码量上看 Chromium 内核有数百万行 C++，考虑到跨平台兼容则过千万行代码，这个规模和复杂度是超过美团 App 本身，即使照着写一遍，没有数十名 C++ 专家投入三五年之功是看不到成效的。第二，当世只有 Google 和 Apple 有这个能力做成 WebView。FireFox 的新浏览器计划半途而废，微软则直接放弃转投 Chromium。\n调用系统 UI 框架：类似 RN 的方式，研发成本上是我们能接受的；渲染样式虽然没有 WebView 的 CSS 全，但是 Flex 足够支撑业务需求了，且保持是 W3C 的严格子集；RN 在性能上的问题主要出在通信层上，这个我们可以解决掉；最后也是最重要的是这个选择不是一个单向门，假如我们获得了驾驭 blink[6] 能力，那么就可以很低的成本平滑切换到 blink 上。\n\n所以渲染层就是复用了 React Native 的 Native 部分，我们决定要站在这个 “巨人” 的肩膀上开始行动。毕竟 React Native 已经提供了非常优秀的组件封装，同时它也解决了 Android 和 iOS 在渲染层面的差异，因为这些接口基本上都是在系统 UI 下进行封装的，所以我们有理由相信这些接口本身的性能是良好的。\n\n然后，剩下的问题就是设置属性、传递属性等成本，它们在实践的过程中，通常会成为页面渲染的一个瓶颈，事实上 React Native 也正在解决这个问题。基于此，我们决定保留下来 React Native 的 UIManager 的增、删、改等概念，以及 Yoga 布局，还有视图组件的封装，我们将这些保留了下来。后面，我们还会再讨论为什么没有使用 React Native 当前的属性转换的方式，这里不再展开讨论。\n\n3.4 整体架构\n\n最终，Recce 的概览如下图所示。这里重点讲下先前没有提及的 Recce Host 平台抽象层，这一层我们主要做了两件事：第一件，属性设置优化（或者叫渲染通信优化）；第二件，平台抽象。\n\n属性设置优化后文会详细介绍，这里只说平台抽象。我们结合 WebIDL 的设计和 LLVM 的架构理解，在平台抽象层上下都实现了标准接口。类似 llvm 的 MIR 使得编译器前端和编译器后端可以独立迭代和接入，平台抽象层的标准接口设计使得只要遵循渲染指令标准的解释器或者渲染器都可以很容易接入。这就是我们很容易把 QuickJS + Vue/React 支持了的原因。Recce-js 可以使线上的大部分以 JavaScript 为主的前端页面获得更好的性能表现。同样 Recce 的鸿蒙适配的成本也非常低，不需要上千或者几千 pd 那么多。最重要的是未来替换性能更好的解释器或者语言或者 UI 框架都是简单可行的。这一层是 Rust 实现的高效、安全且使得容器整体架构易扩展和易维护。\n\n综上选型和搭建工作基本上已经完成了。接下来，我们再对 Recce 上的一些细节问题进行补充。\n\n前面是讲道理，知已不易行更难。下文的 “干货” 可能才是决定成败的关键细节。\n\n4 Recce 的一些细节问题\n\n首先，就是上一节讨提到的，为什么没有使用 React Native 的属性转换？因为，我们发现属性转换是 React Native 一个性能瓶颈。其实为了评估这个问题，我们做了一个 3000 个节点的页面。当然，这个页面可能跟我们平时常见的页面长得不一样，但是它的渲染和布局成本和业务实际是比较接近的。\n\n页面的逻辑我们写得尽可能简单，同时去掉了 React Native 的启动时间，然后我们启用了原生代码，但是调用了 Yoga 的布局计算，就这样写了一个页面去做对比。最终发现，二者的耗时差距非常大，因为我们的布局方式是一样的，调用的 UI 也完全是相同的，基于此，我们基本上就可以认定剩下的 93% 的时间都是为了将设置页面的各种各样的数据从 JavaScript 传递到具体的平台，也就是说属性转换会耗费大量的时间。\n\n这里可以再深挖一下，为什么属性转换会耗费这么多的时间？我们稍微研究了一下这个问题后发现，主要还是因为 React Native 会有多次序列化和反序列化，这是一个类似于字典的东西，而且除了序列化的时间，还需要考虑构建字典本身需要的时间，还有执行字典的一些查找、设置等操作，最终我们还需要频繁地按照字典里面的 Key 值查找，查找到之后，再设置到具体的属性设置，而以上这些操作都会消耗掉不少的时间及内存。\n\n事实上，React Native 官方也正在尝试解决这个问题，在官方公布的的一个实验性的新的框架中，React Native 直接将一个 JSObject 转换成了一个 C++ 的静态类型（*Props），然后在对应的各个平台中，直接使用了一个转换后的静态类型。如此以来，实际上就只存一次 JSObject 这样字典就可以，从创建开始到最终设置，始终使用这个静态类型，那么剩下的操作都会变得非常高效了。\n\n同时，我们也会去思考应该怎么去解决属性传递以及查找的问题。这里可以简单看一下，我们常用的几种数据结构，都是基于数组、链表、哈希表、字典等等之类。但实际上，我们在这种场景下可以选择的可能就只有字典和数组，而 React Native 最常用的方式就是基于字典去构建各种各样的属性。但是基于字典这种方式并没有非常好的性能，如果传递的载体还是 JSON 字符串的时候，还需要承担 JSON 本身解析的任务。经过考量之后，我们最终决定采用基于索引的数组来构建一个个的属性值。\n\n但是采取这种模式的话，维护每个数组属性上的索引将会变得非常的复杂，我们借助一个属性定义生成各端代码维护这个索引，我们在运行前约定好每个属性的索引值，当然，我们会放弃一定的兼容性。其中一个放弃的例子就是，当我们约定好一个属性的索引值之后，之后就不能再修改这个属性的索引值本身，否则就会遇到属性设置可能会发生错乱的问题。\n\n类似的，我们也可以把组件注册的标识从字符串修改为索引，由于这个属性和组件不太一样，就无法知道客户端会提供哪些原生的组件，所以至少要在运行时去使用字符串获取一次组件信息。在获取之后，就可以使用获取的这个索引，从而保持一个比较快的匹配效率。\n\n最近，我们还做了一个富文本的标签，这个标签跟前两者相比就变得更不一样了。这里实际上输入的是一段 HTML 的字符串，所以输入的内容非常自由，我们没有办法像前两者一样使用一些静态的计算方式。但即便是富文本，仍然可以有一些已知的内容，比如像文本的样式、字符串等等，这些内容是可以提前知道的。而在这个层面，我们是可以做一些事情的，实际上可以基于所有输入的计算，得到一个完美的哈希函数，然后确保所有的输入不会发生碰撞。进而，在查找 Key 的时候就会变得快很多。\n\n以上，就是我们在遇到属性传递时解决的各种各样的问题。而接下来，我们还需要解决一个问题——跨语言的调用问题。当然，在讨论这个问题之前，我们先简单地将这些调用划分成了四类：\n\n第一类是 C 与 C 之间的调用，其实 C 语言本身并没有什么转换，这里只是把它作为一个最特殊的场景，进行归类处理。\n第二类是 Rust 去调用 C，这两种语言虽然也是直接在原生上去运行的，但是它们之间会增加了一些调用约定之类的转换。\n第三类是 Java 和 JavaScript 去调用 C，这就需要借助解释器提供的接口去进行调用，其中也会涉及更多的转换的工作，比如说两个内存空间之间的拷贝。\n第四类，我们认为它们其实更接近这个 IPC 的调用，比如像通过 WebView 提供的基于字符串传递的接口。\n\n在这种场景下，我们会有更多的转换工作以及一些编码约定的设置，所以我们必须要找到一种基于字符串编码方式去传递各种复杂的数据。而具体应用到 Recce 内部的时候，其实并不存在类似于 IPC 场景的调用，所以只需要解决每一层之间语言的调用。但实际上，我们仍然面对着非常复杂的调用这个事实。\n\n参考下图，可以看到每一层实际上都涉及到一个具体的跨语言调用。同时，还有刚刚提到的性属性设置，它则会跨过中间的所有层级，直接设置到具体的平台。最后要需要强调一下，涉及到具体的属性以及类似的一些方法调用，比如说打开相机，也会跨过中间层直接去做调用，而这个调用跟属性设置又存在很多不一样的地方。\n\n针对不同的调用场景，我们也采取了两种不同的解决方案：\n\n手写 + 辅助函数：通常用在不会频繁增改接口、手写本身已经比较好维护。\n接口定义 + 代码生成：可能频繁增改接口、手写维护会非常困难（还需要频繁维护文档）。\n\n具体来讲，我们分成了以下 4 个场景：\n\n属性设置：把定义的属性 props_gen 生成 Recce Ul+ Vue + React + Android + iOS+HarmonyOs 等各种属性的操作代码。\nRust（Wasm3）：扩展 wit-bindgen支持 Wasm3。\nQuickJS：借助 Rust 宏封装一些本地函数（UI 操作接口）+ 自定义的二进制读写实现。\n业务方法调用 & 平台抽象层与 Platform 交互：使用 mako完成接口调用和文档生成。\n5 总结和展望\n\n最终我们获得了一个如上图的高性能、安全的动态化容器，可以以 Wasm 的方式支持原生级别的性能，也可以将 JavaScript 的前端工程的性能提升一截。\n\n从某个角度看，像是我们把 RN 用 Rust 重写了，添加了 Wasm 解释器的支持。但用熟悉 WebView 架构的视角看，也可以看作是一个 WebEngine Lite，只是试图绘制暂时用的系统 UI。\n\n文章最后做一下回望和展望。\n\n回望：我们所做的所有架构和优化工作都可以概括为，区分本质复杂度和偶然复杂度，恰当的回应本质复杂度，降低偶然复杂度。\n\n动态化容器的本质复杂度是什么？最主要的一条脉络是，渲染管线，以前端研发的编码逻辑和数据为输入，在管线中变为组件树 -> 虚拟文档树 -> 文档树 -> 视图树 / 样式树 -> 图层树 -> 呈现树，最终绘制到屏幕上为用户所眼见视图为输出。至于用什么 DSL、编程语言、解释器、编码方式等等其实是偶然复杂度。\n\nRecce 的选型和搭建过程，实际上是围绕渲染管线进行优化，比如精简流程去掉了 VDOM 等环节，比如简化运行时、选用执行效率更高的解释器和编码方式等，再比如削峰填谷、消除瓶颈以提升渲染管线整体效率等。而这些工作都是为了降低容器本身的计算开销，因为动态化容器相对业务而言也是偶然复杂度。将终端有限的软硬件计算资源更多的留给业务本质复杂度是容器迭代的正确方向。\n\n展望：目前 Recce 还在逐步完善和落地推广阶段，可以做的事情有很多：\n\n改善开发体验：比如引入 LLM 来降低 Rust 的学习门槛和开发成本，比如完善调试 & 脚手架工具链等等。\n进一步优化性能：比如以 Rust 原生方式运行以获得超过 Android 原生的性能表现；比如利用 Wasm 解释器线性内存特性，可以在 CI 上完成大部分的预计算，进一步提升加载性能表现；\n自研渲染层：这样一是能进一步提高性能，二也能降低多端维护成本，三是把样式能力集对齐到 WebView 可以实现和 H5、小程序的同构。当然这样其实就是 一个完整的 WebEngine Lite 了。\n6 Q & A\n\nQ：Recce 的性能如何？可以把这个问题更具象化一些吗？\n\nA：目前从我们已经实践的范围中，在我们业务场景中能够找到最低端的 POS 设备上面，Recce UI 是可以和 Flutter 的性能表现媲美，而且我们是在动态解释运行，而 Flutter 是原生运行。也就是实现了原生级别的性能表现。\n\nQ：以后 JS 没用了吗？\n\nA：JS 有用的，但这个问题分两个层面回答。第一层，Recce 是支持这个 JS 运行时和相应生态的。比较极端的场景，比如说 POS 机，下沉市场的低端机，或者说对于性能要求很高的，比如说像 App 的首页冷启动，或者说像这个支付收银台等等这种场景下，优化性能的收益很大，那么就可以考虑用 Recce-rs 这个方案。如果不是这么极端的一般场景，我们用这个 Recce-js 的方案也能获得一个低成本，获得一个性能优化。性能优化是没有止境的，根据业务场景的需要去选择，Recce 提供了更多且更好的选择。\n\n第二层，移动端开发从一开始就有原生开发和 H5/JS 开发（PhoneGap 发布于 2009 年），但 JS 成为主流，始于 2015 年，Google 把 V8 适配到了 Android 上。这里就不是 JS 行不行，而是 V8 很行。 终端硬件的计算资源始终是有限的，V8 通过解释器层面 JIT/AOT 编译技术 大幅提升了 JS 的运行效率，使得 JS 铺开成为可能。但近几年 JS 引擎不再有大幅度迭代，手机硬件算力的升级速度也明显放缓，这就导致了终端上 软硬件计算容量的提升速度跟不上业务复杂度的提升速度。这才是当下 JS 技术栈面临的问题，也是为什么有如此多 JS 框架在卷性能的原因。\n\n当然大家不用特别悲观，JS 和 Web 始终是互联网最重要的基础设施，事物是呈现螺旋发展趋势的（当然有人负责发展，有人负责螺旋），随着周期演变，新的软硬件技术升级又会推动 JavaScript 往前发展。比如如果一个使用 TypeScript/JavaScript 的高性能解释器出来，那么能够使现在的 JavaScript 工程性能大幅度提升，毕竟前端线上代码资产 90% 是以 JavaScript 形式存在的。\n\nQ：为什么叫 Recce ？\n\nA：Recce/ˈrɛkiː/ 名字取自海豹部队的一把枪。选择这个名字的初衷是因为在 Recce-rs 选型的时候，目标是接近原生的性能表现、还要动态化，那么肯定要舍弃一部分开发体验的，切换 Rust 语言上会对研发同学增加学习成本、抬高门槛。因为预判到这一点，所以我们希望使用 Recce 的同学始终记得是经过了更多的训练的精英，要克服各种困难去完成高价值的任务。凡事有利必有弊，有用比有费，对于需要使用 Recce-rs 优化的场景 学习 Rust 就不是最难的技术问题。我们在为高价值场景提供更好的选择的同时，也将部分优化能力通过 Recce-js 反哺到一般业务场景上。不同的场景不同的成本结构，没有最好，只有合适的方案。\n\nQ：Recce 为什么要追求原生级别的性能？\n\nA：主要有三点原因。\n\n一、前端提升性能体验可以提升用户体验和业务获客效率\n\n前端部署和面客是重叠的过程，前端部署成功率影响业务面客。部署成功率低意味着，客户遇到白屏或不愿意等待最终放弃。提升性能可以提升前端部署成功率，进而可以提升用户体验和业务获客效率。\n\n部署：代码离开开发环境，到用户终端设备运行前的这段环节为部署。\n\n二、追求原生级别性能的动态化容器是公司业务的发展需要\n\n美团业务的首要特点是低毛利，低毛利意味着业务发展需要做大规模，提升 UE，提升复购。\n\n做大规模，意味着要争取广大下沉市场。那么在性能体验上就需要至少 Meet 友商，在下沉市场表现最好的应用（微信、拼多多）的主要功能都是原生实现的。争取下沉市场扩大规模在前端的命题意味着提供原生级别的性能体验，在下沉市场这可能不再是体验好坏而是能否使用的问题。\n提升 UE，意味着必须要考虑研发人效，前端提研发人效就必须考虑跨平台同构，因此需要容器化，将多端开发降为一端开发。\n提升复购，复购意味着多业态混合经营，那么在包大小约束下就必须要让大部分业务动态化。\n\n综上，公司业务发展的需要决定了美团需要原生级别性能的动态化容器。\n\n三、部署是前端工程领域的核心问题，容器是部署问题的主要答案\n\n把时间拨回到 2010 年，彼时移动端开发方兴未艾，主要的技术栈只有 Native 和 H5（PhoneGap 为代表），其中 Native 性能表现好但不跨端、H5 迭代效率高但性能体验差。随着时间推移，Native 方向为了解决跨端研发成本和部署效率，发展出了 ReactNative、Flutter 等等方案，H5 方向为了改善性能，发展出了离线化、SSR/ESR、解释器优化等等。\n\n可以说两个方向是相向而行，其实这是一个问题的两个面。究其根本在于前端代码运行在不受我们控制且计算资源有限的用户设备上，不同于后端的主要问题是高并发，前端的主要问题是怎样把代码低成本跨过物权边界送到大规模且不同的用户的设备上并高效运行起来。\n\n回顾过去，性能表现好原生要做动态化降低部署成本，部署成本低的 H5 要优化性能提升用户体验。换句话说，前端工程领域的核心问题是部署成本和用户体验的平衡。比如包大小就是原生开发部署成本高而非常困扰客户端开发的一个典型例子。\n\n注释\n[1] 逻辑解释器：在 iOS 中 Apple 不允许 JIT，只可以解释执行。\n[2] WasmEdge：亚马逊用做云原生的 Wasm 解释器。\n[3] C#：黑神话悟空就是用魔改 USharp 跑的游戏逻辑。\n[4] 不支持 JIT ：Apple 不允许在 iOS 上 JIT。\n[5] C、C++、Rust、Go 这四种\n[6] blink：chromium 的布局渲染器。","str":"第二层，移动端开发从一开始就有原生开发和 H5/JS 开发（PhoneGap 发布于 2009 年），但 JS 成为主流，始于 2015 年，Google 把 V8 适配到了 Android 上。这里就不是 JS 行不行，而是 V8 很行。 终端硬件的计算资源始终是有限的，V8 通过解释器层面 JIT/AOT 编译技术 大幅提升了 JS 的运行效率，使得 JS 铺开成为可能。但近几年 JS 引擎不再有大幅度迭代，手机硬件算力的升级速度也明显放缓，这就导致了终端上 软硬件计算容量的提升速度跟不上业务复杂度的提升速度。这才是当下 JS 技术栈面临的问题，也是为什么有如此多 JS 框架在卷性能的原因。"}],"note":"","style":"0","tags":[],"text":"第二层，移动端开发从一开始就有原生开发和 H5/JS 开发（PhoneGap 发布于 2009 年），但 JS 成为主流，始于 2015 年，Google 把 V8 适配到了 Android 上。这里就不是 JS 行不行，而是 V8 很行。 终端硬件的计算资源始终是有限的，V8 通过解释器层面 JIT/AOT 编译技术 大幅提升了 JS 的运行效率，使得 JS 铺开成为可能。但近几年 JS 引擎不再有大幅度迭代，手机硬件算力的升级速度也明显放缓，这就导致了终端上 软硬件计算容量的提升速度跟不上业务复杂度的提升速度。这才是当下 JS 技术栈面临的问题，也是为什么有如此多 JS 框架在卷性能的原因。","type":"paragraph"}],"create":"2025年02月17日 12:09:10","desc":"长久以来，容器要实现动态化和双端复用，难免要牺牲掉一些性能。有没有办法让动态化容器的性能尽可能接近原生？美团金服大前端团队给出了一种解决方案，尝试突破动态化容器的天花板。","favicon":"https://awps-assets.meituan.net/mit/blog/v20190629/asset/icon/android-icon-192x192.png?v=Whistle&t=20181017-1r","idx":1,"img":"https://p0.meituan.net/meituantechblog/31ce13d9379f94f5b526304dbd1cd8321161128.png","note":"","tags":[],"title":"大前端：如何突破动态化容器的天花板？ - 美团技术团队","url":"https://tech.meituan.com/2024/10/18/recce-in-meituan.html"},{"annotations":[{"annote":"<sr-annote data-id=\"1739767171057\">Casually Coding With An LLM</sr-annote>","color":1,"html":"Casually Coding With An LLM","id":1739767171057,"items":[{"root":"Despite researching and developing tooling around LLMs even long before ChatGPT, I haven’t been fond of using LLM code copilots such as GitHub Copilot for coding assistance. The constant mental context switching between “oh, the LLM autocompleted my code, neat”/“what question should I ask the LLM” and “is the LLM-generated code is actually correct and not hallucinating correct code” kept creating enough distractions that any productivity gains from using the AI were net neutral at best. That’s also disregarding the expensive cost of using said LLMs.","str":"Casually Coding With An LLM"}],"note":"","style":"0","tags":[],"text":"Casually Coding With An LLM","type":"paragraph"},{"annote":"<sr-annote data-id=\"1739767171059\">Initial Ask</sr-annote>","color":1,"html":"Initial Ask","id":1739767171059,"items":[{"root":"For this experiment, we will give Claude 3.5 Sonnet an interview-style coding prompt using Python: one that is both simple and could be implemented by a novice software engineer, but one that can be heavily optimized. This simple, casual prompt represents how the typical software engineer uses LLMs. Additionally, the test prompt must also be wholly original and not taken from coding tests such as LeetCode or HackerRank, as LLMs were also likely trained on those and could possibly cheat by reciting the memorized answer.","str":"Initial Ask"}],"note":"","style":"0","tags":[],"text":"Initial Ask","type":"paragraph"},{"annote":"<sr-annote data-id=\"1739767171061\">Iteration #1</sr-annote>","color":1,"html":"Iteration #1","id":1739767171061,"items":[{"root":"Now we can ask Claude to improve the code by including its current answer and all that came before it as a part of the conversation prompt. Thus, we add the iteration prompt:","str":"Iteration #1"}],"note":"","style":"0","tags":[],"text":"Iteration #1","type":"paragraph"},{"annote":"<sr-annote data-id=\"1739767171062\">Iteration #2</sr-annote>","color":1,"html":"Iteration #2","id":1739767171062,"items":[{"root":"After another write better code, Claude discovers more obvious optimizations (truncated for readabilty):","str":"Iteration #2"}],"note":"","style":"0","tags":[],"text":"Iteration #2","type":"paragraph"},{"annote":"<sr-annote data-id=\"1739767171064\">Iteration #3</sr-annote>","color":1,"html":"Iteration #3","id":1739767171064,"items":[{"root":"Another write better code, and Claude returns a implementation that it claims is “even more sophisticated and optimized version using advanced techniques and modern Python features” but the actual code shows no significant algorithmic improvements and actually a regression in the digit sum calculation by reverting back to the type-casting approach. If anything, the codebase is becoming more bloated, such as adding a class for performing the difference:","str":"Iteration #3"}],"note":"","style":"0","tags":[],"text":"Iteration #3","type":"paragraph"},{"id":1739790224714,"color":1,"style":"0","tags":[],"note":"","type":"paragraph","html":"anything","text":"anything","annote":"<sr-annote data-id=\"1739790224714\">anything</sr-annote>","items":[{"str":"anything","root":"Another write better code, and Claude returns a implementation that it claims is “even more sophisticated and optimized version using advanced techniques and modern Python features” but the actual code shows no significant algorithmic improvements and actually a regression in the digit sum calculation by reverting back to the type-casting approach. If anything, the codebase is becoming more bloated, such as adding a class for performing the difference:"}]},{"annote":"<sr-annote data-id=\"1739767171065\">Iteration #4</sr-annote>","color":1,"html":"Iteration #4","id":1739767171065,"items":[{"root":"This iterative prompting appears to be hitting diminishing returns. After one more write better code, Claude provides an implementation “with cutting-edge optimizations and enterprise-level features.” Wait, enterprise-level features?!","str":"Iteration #4"}],"note":"","style":"0","tags":[],"text":"Iteration #4","type":"paragraph"},{"annote":"<sr-annote data-id=\"1739767171067\">Prompt Engineering LLMs For Even More Better Code</sr-annote>","color":1,"html":"Prompt Engineering LLMs For Even More Better Code","id":1739767171067,"items":[{"root":"It’s 2025, and prompt engineering LLMs is still required to get best results from them. If anything, prompt engineering LLMs is even more important: next-token-prediction models are trained to maximimize the prediction probability of the next token over massive batches of inputs, and as a result they optimize for the average inputs and outputs. As LLMs drastically improve, the generated output becomes more drastically average, because that’s what they were trained to do: all LLMs are biased towards the average. Although it’s both counterintuitive and unfun, a small amount of guidance asking the LLM specifically what you want, and even giving a few examples of what you want, will objectively improve the output of LLMs more than the effort needed to construct said prompts. Claude 3.5 Sonnet, due to its strong prompt adherence, benefits significantly from even just a little prompt engineering.","str":"Prompt Engineering LLMs For Even More Better Code"}],"note":"","style":"0","tags":[],"text":"Prompt Engineering LLMs For Even More Better Code","type":"paragraph"},{"annote":"<sr-annote data-id=\"1739767171069\">Initial Ask</sr-annote>","color":1,"html":"Initial Ask","id":1739767171069,"items":[{"root":"This time we will use a system prompt, only available via an API. The system prompt lists the LLM’s “rules” it must follow. Since I want more optimized code, we’ll define that in the rules, with granular examples:","str":"Initial Ask"}],"note":"","style":"0","tags":[],"text":"Initial Ask","type":"paragraph"},{"annote":"<sr-annote data-id=\"1739767171070\">Iteration #1</sr-annote>","color":1,"html":"Iteration #1","id":1739767171070,"items":[{"root":"We can now ask Claude to iterate on the code with a more verbose prompt than “write code better”:","str":"Iteration #1"}],"note":"","style":"0","tags":[],"text":"Iteration #1","type":"paragraph"},{"annote":"<sr-annote data-id=\"1739767171072\">Iteration #2</sr-annote>","color":1,"html":"Iteration #2","id":1739767171072,"items":[{"root":"Another iteration of the prompt:","str":"Iteration #2"}],"note":"","style":"0","tags":[],"text":"Iteration #2","type":"paragraph"},{"annote":"<sr-annote data-id=\"1739767171073\">Iteration #3</sr-annote>","color":1,"html":"Iteration #3","id":1739767171073,"items":[{"root":"Another iteration:","str":"Iteration #3"}],"note":"","style":"0","tags":[],"text":"Iteration #3","type":"paragraph"},{"annote":"<sr-annote data-id=\"1739767171075\">Iteration #4</sr-annote>","color":1,"html":"Iteration #4","id":1739767171075,"items":[{"root":"At this point, Claude actually complained that the code is at the “theoretical minimum time complexity possible for this problem.” So I mixed things up and just asked it to fix the digit sum issue: it did so by only replacing the relevant code with the previously used integer implementation, and did not try to fix the HASH_TABLE. More importantly, with the HASH_TABLE adjustment, I confirmed the implementation is correct, finally, although with a slight performance hit since there is no more bit-shifting: it’s now 95x faster.","str":"Iteration #4"}],"note":"","style":"0","tags":[],"text":"Iteration #4","type":"paragraph"},{"annote":"<sr-annote data-id=\"1739767171077\">Next Steps For Better LLM Code Generation</sr-annote>","color":1,"html":"Next Steps For Better LLM Code Generation","id":1739767171077,"items":[{"root":"Putting it all together, let’s visualize the improvements, including highlighting the cases where I needed to alter the logic of the code to make it runnable due to bugs.","str":"Next Steps For Better LLM Code Generation"}],"note":"","style":"0","tags":[],"text":"Next Steps For Better LLM Code Generation","type":"paragraph"}],"create":"2025年02月17日 11:41:02","desc":"Most coders want AI to write code faster: I want AI to write FASTER CODE.","favicon":"https://minimaxir.com/favicon.ico","idx":0,"img":"https://minimaxir.com/2025/01/write-better-code/featured.png","note":"","tags":[],"title":"Can LLMs write better code if you keep asking them to “write better code”? | Max Woolf's Blog","url":"https://minimaxir.com/2025/01/write-better-code/?ref=dailydev"}],"collections":[],"plugstorge":{},"secret":{"dropbox":{"access_token":""},"evernote":{"access_token":""},"flomo":{"access_token":""},"focusnote":{"access_token":""},"gdrive":{"access_token":"","folder_id":""},"github":{"access_token":"","path":"","repo":""},"hypothes":{"access_token":"","group_id":""},"instapaper":{"access_token":"","token_secret":""},"jianguo":{"access_token":"","password":"","username":""},"joplin":{"access_token":"","folder_id":""},"linnk":{"access_token":"","group_name":""},"notion":{"access_token":"","folder_id":"","save_image":false,"type":""},"onenote":{"access_token":""},"pocket":{"access_token":"","tags":""},"version":"2021-01-19","webdav":[],"weizhi":{"access_token":"","folder":"","host":"","kbGuid":"","kbServer":"","password":"","username":""},"yinxiang":{"access_token":""},"youdao":{"access_token":"","folder_id":""},"yuque":{"access_token":"","repos_id":""}}}